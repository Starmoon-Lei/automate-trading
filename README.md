# ğŸš€ å…è´¹äº‘æœåŠ¡AIäº¤æ˜“ç³»ç»Ÿå®Œæ•´æ‰§è¡ŒæŒ‡å—

## ğŸ“‹ é¡¹ç›®æ¦‚è§ˆ

### ç³»ç»Ÿæ¶æ„ - 100%å…è´¹ç‰ˆ
```
å°çº¢ä¹¦/Twitter â†’ Railway.app â†’ OpenAIè§£æ â†’ ä¿¡å·åˆ†çº§ â†’ TradingView/Gmailé€šçŸ¥
     â†“             â†“              â†“           â†“              â†“
   çˆ¬è™«ç›‘æ§     Python FastAPI   æ™ºèƒ½è§£æ   é«˜ç½®ä¿¡åº¦æ‰§è¡Œ    ä½ç½®ä¿¡åº¦é‚®ä»¶
   (å…è´¹)        (500å°æ—¶/æœˆ)      (æˆæœ¬æ§åˆ¶)  (è‡ªåŠ¨äº¤æ˜“)     (äººå·¥å®¡æ ¸)
```

### å…è´¹æŠ€æœ¯æ ˆ
- **æœåŠ¡å™¨**: Railway.app (500æ‰§è¡Œå°æ—¶/æœˆ)
- **æ•°æ®åº“**: Supabase (500MB PostgreSQL)
- **ç¼“å­˜**: Railway.app Redis (å†…ç½®)
- **å­˜å‚¨**: Supabase Storage (1GB)
- **é‚®ä»¶**: Gmail SMTP (500å°/å¤©)
- **ç›‘æ§**: UptimeRobot (50ä¸ªç›‘æ§ç‚¹)
- **ä»£ç ä»“åº“**: GitHub (ç§æœ‰ä»“åº“)

### æ ¸å¿ƒåŠŸèƒ½
- âœ… **æ™ºèƒ½è§£æ**: OpenAI GPT-4è§£æç¤¾åª’å†…å®¹
- âœ… **åˆ†çº§å¤„ç†**: é«˜ç½®ä¿¡åº¦è‡ªåŠ¨æ‰§è¡Œï¼Œä½ç½®ä¿¡åº¦é‚®ä»¶é€šçŸ¥
- âœ… **æˆæœ¬æ§åˆ¶**: è´¨é‡é¢„ç­›é€‰ï¼Œæœˆé¢„ç®—æ§åˆ¶
- âœ… **é£é™©ç®¡ç†**: å¤šå±‚å®‰å…¨æ£€æŸ¥
- âœ… **å®æ—¶ç›‘æ§**: ç³»ç»ŸçŠ¶æ€å’Œæ€§èƒ½ç›‘æ§

---

## ğŸ¯ ç¬¬ä¸€é˜¶æ®µï¼šå…è´¹æœåŠ¡æ³¨å†Œå’Œé…ç½®ï¼ˆç¬¬1-2å¤©ï¼‰

### 1.1 å…è´¹æœåŠ¡è´¦å·ç”³è¯·

**å¿…éœ€çš„å…è´¹è´¦å·**:
```bash
# 1. æ ¸å¿ƒæœåŠ¡
âœ… Railway.app - ä¸»æœåŠ¡å™¨
âœ… Supabase.com - PostgreSQLæ•°æ®åº“
âœ… GitHub.com - ä»£ç ä»“åº“
âœ… OpenAI.com - AIè§£æAPI
âœ… UptimeRobot.com - ç›‘æ§æœåŠ¡

# 2. å¯é€‰æœåŠ¡
âœ… TradingView.com - äº¤æ˜“æ‰§è¡Œ
âœ… Gmail.com - é‚®ä»¶æœåŠ¡
```

### 1.2 OpenAI APIé…ç½®

```python
# è·å–OpenAI APIå¯†é’¥
# 1. è®¿é—® https://platform.openai.com/api-keys
# 2. åˆ›å»ºæ–°çš„APIå¯†é’¥
# 3. è®¾ç½®ä½¿ç”¨é™åˆ¶ï¼š$50/æœˆ
# 4. è®°å½•APIå¯†é’¥ï¼Œåç»­é…ç½®ä½¿ç”¨

# æˆæœ¬æ§åˆ¶è®¾ç½®
OPENAI_MONTHLY_BUDGET = 50.0  # ç¾å…ƒ
OPENAI_MODEL = "gpt-4o-mini"  # æœ€ä¾¿å®œçš„GPT-4æ¨¡å‹
```

### 1.3 Supabaseæ•°æ®åº“é…ç½®

```sql
-- 1. åˆ›å»ºSupabaseé¡¹ç›®
-- 2. è·å–æ•°æ®åº“è¿æ¥ä¿¡æ¯
-- 3. åˆ›å»ºæ•°æ®è¡¨ç»“æ„

-- åšä¸»å¸–å­è¡¨
CREATE TABLE blogger_posts (
    id SERIAL PRIMARY KEY,
    platform VARCHAR(50) NOT NULL,
    post_id VARCHAR(100) UNIQUE NOT NULL,
    author VARCHAR(100),
    content TEXT,
    quality_score FLOAT DEFAULT 0.0,
    ai_parsed BOOLEAN DEFAULT false,
    extracted_signals JSONB,
    ai_analysis JSONB,
    timestamp TIMESTAMP DEFAULT NOW(),
    processed BOOLEAN DEFAULT false
);

-- AIä¿¡å·è¡¨
CREATE TABLE ai_signals (
    id SERIAL PRIMARY KEY,
    post_id VARCHAR(100) NOT NULL,
    symbol VARCHAR(10) NOT NULL,
    company_name VARCHAR(100),
    action VARCHAR(10),
    confidence FLOAT,
    credibility_score FLOAT,
    target_price FLOAT,
    current_price FLOAT,
    reasoning TEXT,
    risk_level VARCHAR(20),
    time_horizon VARCHAR(20),
    status VARCHAR(20) DEFAULT 'PENDING',
    execution_method VARCHAR(20),
    timestamp TIMESTAMP DEFAULT NOW()
);

-- äº¤æ˜“è®°å½•è¡¨
CREATE TABLE trades (
    id SERIAL PRIMARY KEY,
    signal_id INTEGER REFERENCES ai_signals(id),
    symbol VARCHAR(10),
    action VARCHAR(10),
    quantity FLOAT,
    entry_price FLOAT,
    stop_loss FLOAT,
    take_profit FLOAT,
    tradingview_alert_id VARCHAR(100),
    status VARCHAR(20),
    timestamp TIMESTAMP DEFAULT NOW()
);

-- ç³»ç»ŸæŒ‡æ ‡è¡¨
CREATE TABLE system_metrics (
    id SERIAL PRIMARY KEY,
    date TIMESTAMP DEFAULT NOW(),
    openai_requests INTEGER DEFAULT 0,
    openai_cost FLOAT DEFAULT 0.0,
    signals_generated INTEGER DEFAULT 0,
    high_confidence_signals INTEGER DEFAULT 0,
    trades_executed INTEGER DEFAULT 0,
    email_alerts_sent INTEGER DEFAULT 0
);

-- åˆ›å»ºç´¢å¼•æé«˜æŸ¥è¯¢æ€§èƒ½
CREATE INDEX idx_blogger_posts_timestamp ON blogger_posts(timestamp);
CREATE INDEX idx_ai_signals_confidence ON ai_signals(confidence);
CREATE INDEX idx_trades_timestamp ON trades(timestamp);
```

---

## ğŸ’» ç¬¬äºŒé˜¶æ®µï¼šæ ¸å¿ƒä»£ç å®ç°ï¼ˆç¬¬3-7å¤©ï¼‰

### 2.1 é¡¹ç›®ç»“æ„å’Œä¾èµ–

```
trading-system/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                 # FastAPIä¸»åº”ç”¨
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ settings.py         # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ database.py         # æ•°æ®æ¨¡å‹
â”‚   â”œâ”€â”€ parsers/
â”‚   â”‚   â””â”€â”€ openai_parser.py    # OpenAIè§£æå™¨
â”‚   â”œâ”€â”€ monitors/
â”‚   â”‚   â””â”€â”€ social_monitor.py   # ç¤¾åª’ç›‘æ§
â”‚   â”œâ”€â”€ traders/
â”‚   â”‚   â””â”€â”€ tradingview_client.py # TradingViewé›†æˆ
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ email_service.py    # é‚®ä»¶æœåŠ¡
â”‚   â”‚   â””â”€â”€ quality_filter.py   # è´¨é‡è¿‡æ»¤
â”‚   â””â”€â”€ trading_engine.py       # æ ¸å¿ƒäº¤æ˜“å¼•æ“
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ railway.json
â””â”€â”€ README.md
```

### 2.2 ä¾èµ–é…ç½®ï¼ˆä¼˜åŒ–å…è´¹ç‰ˆï¼‰

```txt
# requirements.txt - ä¼˜åŒ–å†…å­˜ä½¿ç”¨
fastapi==0.104.1
uvicorn==0.24.0
psycopg2-binary==2.9.9
sqlalchemy==2.0.23
pydantic==2.5.0

# AIå’Œè§£æ
openai==1.35.0
tiktoken==0.7.0
tenacity==8.2.3

# ç½‘é¡µæŠ“å–ï¼ˆè½»é‡çº§ï¼‰
requests==2.31.0
beautifulsoup4==4.12.2
selenium==4.15.0

# é‚®ä»¶å’Œå·¥å…·
aiosmtplib==3.0.1
python-dotenv==1.0.0
schedule==1.2.0

# ä¼˜åŒ–åŒ…ï¼ˆå‡å°‘å†…å­˜ä½¿ç”¨ï¼‰
asyncpg==0.29.0  # æ›¿ä»£psycopg2ï¼Œæ›´é«˜æ•ˆ
httpx==0.25.2    # æ›¿ä»£requestsï¼Œå¼‚æ­¥æ”¯æŒ
```

### 2.3 é…ç½®ç®¡ç†ï¼ˆå…è´¹ç‰ˆä¼˜åŒ–ï¼‰

```python
# src/config/settings.py
import os
from pydantic_settings import BaseSettings
from typing import List

class Settings(BaseSettings):
    # åº”ç”¨é…ç½®
    APP_NAME: str = "Free AI Trading System"
    DEBUG: bool = False
    
    # æ•°æ®åº“é…ç½® - Supabase
    DATABASE_URL: str = os.getenv("DATABASE_URL", "")
    
    # OpenAIé…ç½® - æˆæœ¬æ§åˆ¶
    OPENAI_API_KEY: str = os.getenv("OPENAI_API_KEY", "")
    OPENAI_MODEL: str = "gpt-4o-mini"  # æœ€ä¾¿å®œçš„æ¨¡å‹
    OPENAI_MAX_TOKENS: int = 2000       # å‡å°‘tokenä½¿ç”¨
    OPENAI_MONTHLY_BUDGET: float = 50.0 # æœˆåº¦é¢„ç®—æ§åˆ¶
    
    # TradingViewé…ç½®
    TRADINGVIEW_WEBHOOK_URL: str = os.getenv("TRADINGVIEW_WEBHOOK_URL", "")
    
    # Gmail SMTPé…ç½®
    SMTP_HOST: str = "smtp.gmail.com"
    SMTP_PORT: int = 587
    SMTP_USERNAME: str = os.getenv("SMTP_USERNAME", "")
    SMTP_PASSWORD: str = os.getenv("SMTP_PASSWORD", "")
    ALERT_EMAIL: str = os.getenv("ALERT_EMAIL", "")
    
    # äº¤æ˜“é…ç½®
    HIGH_CONFIDENCE_THRESHOLD: float = 0.75
    MEDIUM_CONFIDENCE_THRESHOLD: float = 0.5
    MIN_CREDIBILITY_SCORE: float = 0.6
    MAX_POSITION_SIZE: float = 1000.0
    STOP_LOSS_PERCENT: float = 0.05
    TAKE_PROFIT_PERCENT: float = 0.10
    
    # ç›‘æ§é…ç½®
    BLOGGER_IDS: List[str] = ["blogger1", "blogger2"]
    CHECK_INTERVAL_MINUTES: int = 10  # å‡å°‘åˆ°10åˆ†é’Ÿï¼ŒèŠ‚çœèµ„æº
    QUALITY_THRESHOLD: float = 0.6
    
    # å…è´¹ç‰ˆé™åˆ¶
    MAX_DAILY_TRADES: int = 5        # å‡å°‘äº¤æ˜“é¢‘ç‡
    MAX_DAILY_LOSS: float = 200.0    # é™ä½é£é™©é™åˆ¶
    MAX_MEMORY_MB: int = 450         # Railwayå…è´¹ç‰ˆå†…å­˜é™åˆ¶
    
    class Config:
        env_file = ".env"

settings = Settings()
```

### 2.4 æ•°æ®åº“æ¨¡å‹ï¼ˆè½»é‡çº§ï¼‰

```python
# src/models/database.py
from sqlalchemy import Column, Integer, String, Float, DateTime, Boolean, Text, JSON
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from sqlalchemy import create_engine
from datetime import datetime
import asyncpg
import asyncio
from src.config.settings import settings

Base = declarative_base()

class BloggerPost(Base):
    __tablename__ = "blogger_posts"
    
    id = Column(Integer, primary_key=True, index=True)
    platform = Column(String(50), nullable=False)
    post_id = Column(String(100), unique=True, nullable=False)
    author = Column(String(100))
    content = Column(Text)
    quality_score = Column(Float, default=0.0)
    ai_parsed = Column(Boolean, default=False)
    extracted_signals = Column(JSON)
    ai_analysis = Column(JSON)
    timestamp = Column(DateTime, default=datetime.utcnow)
    processed = Column(Boolean, default=False)

class AISignal(Base):
    __tablename__ = "ai_signals"
    
    id = Column(Integer, primary_key=True, index=True)
    post_id = Column(String(100), nullable=False)
    symbol = Column(String(10), nullable=False)
    company_name = Column(String(100))
    action = Column(String(10))
    confidence = Column(Float)
    credibility_score = Column(Float)
    target_price = Column(Float)
    current_price = Column(Float)
    reasoning = Column(Text)
    risk_level = Column(String(20))
    time_horizon = Column(String(20))
    status = Column(String(20), default="PENDING")
    execution_method = Column(String(20))
    timestamp = Column(DateTime, default=datetime.utcnow)

class Trade(Base):
    __tablename__ = "trades"
    
    id = Column(Integer, primary_key=True, index=True)
    signal_id = Column(Integer)
    symbol = Column(String(10))
    action = Column(String(10))
    quantity = Column(Float)
    entry_price = Column(Float)
    stop_loss = Column(Float)
    take_profit = Column(Float)
    tradingview_alert_id = Column(String(100))
    status = Column(String(20))
    timestamp = Column(DateTime, default=datetime.utcnow)

class SystemMetrics(Base):
    __tablename__ = "system_metrics"
    
    id = Column(Integer, primary_key=True, index=True)
    date = Column(DateTime, default=datetime.utcnow)
    openai_requests = Column(Integer, default=0)
    openai_cost = Column(Float, default=0.0)
    signals_generated = Column(Integer, default=0)
    high_confidence_signals = Column(Integer, default=0)
    trades_executed = Column(Integer, default=0)
    email_alerts_sent = Column(Integer, default=0)

# ä¼˜åŒ–çš„æ•°æ®åº“è¿æ¥ï¼ˆå…è´¹ç‰ˆï¼‰
engine = create_engine(
    settings.DATABASE_URL,
    pool_size=3,        # å‡å°‘è¿æ¥æ± å¤§å°
    max_overflow=2,     # å‡å°‘æº¢å‡ºè¿æ¥
    pool_timeout=30,
    pool_recycle=1800,  # 30åˆ†é’Ÿå›æ”¶è¿æ¥
    echo=False          # å…³é—­SQLæ—¥å¿—èŠ‚çœå†…å­˜
)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

# å¼‚æ­¥æ•°æ®åº“è®¿é—®ï¼ˆæ›´é«˜æ•ˆï¼‰
class AsyncDatabaseManager:
    def __init__(self):
        self.pool = None
    
    async def connect(self):
        if not self.pool:
            self.pool = await asyncpg.create_pool(
                settings.DATABASE_URL,
                min_size=1,      # æœ€å°è¿æ¥æ•°
                max_size=5,      # æœ€å¤§è¿æ¥æ•°ï¼Œå…è´¹ç‰ˆé™åˆ¶
                command_timeout=60
            )
    
    async def disconnect(self):
        if self.pool:
            await self.pool.close()
    
    async def execute_query(self, query: str, *args):
        if not self.pool:
            await self.connect()
        
        async with self.pool.acquire() as connection:
            return await connection.fetchrow(query, *args)
    
    async def execute_many(self, query: str, args_list):
        if not self.pool:
            await self.connect()
        
        async with self.pool.acquire() as connection:
            return await connection.executemany(query, args_list)

db_manager = AsyncDatabaseManager()
```

### 2.5 OpenAIè§£æå™¨ï¼ˆæˆæœ¬ä¼˜åŒ–ç‰ˆï¼‰

```python
# src/parsers/openai_parser.py
import openai
import json
import tiktoken
import asyncio
import aiohttp
from typing import List, Dict, Optional, Any
from pydantic import BaseModel, Field, validator
from tenacity import retry, stop_after_attempt, wait_exponential
import logging
from datetime import datetime
import re
from src.config.settings import settings

# ç²¾ç®€çš„æ•°æ®æ¨¡å‹
class StockSignal(BaseModel):
    symbol: str = Field(..., description="è‚¡ç¥¨ä»£ç ")
    action: str = Field(..., description="äº¤æ˜“åŠ¨ä½œ: BUY, SELL, HOLD")
    confidence: float = Field(..., ge=0.0, le=1.0, description="ç½®ä¿¡åº¦")
    target_price: Optional[float] = Field(None, description="ç›®æ ‡ä»·æ ¼")
    reasoning: str = Field(..., description="æ¨èç†ç”±")
    risk_level: str = Field(..., description="é£é™©ç­‰çº§: LOW, MEDIUM, HIGH")

class ParsedContent(BaseModel):
    original_text: str
    platform: str
    post_id: str
    author: str
    timestamp: datetime
    signals: List[StockSignal]
    sentiment: str = Field(..., description="æ•´ä½“æƒ…ç»ª: BULLISH, BEARISH, NEUTRAL")
    credibility_score: float = Field(..., ge=0.0, le=1.0, description="å†…å®¹å¯ä¿¡åº¦")

class OptimizedOpenAIParser:
    """æˆæœ¬ä¼˜åŒ–çš„OpenAIè§£æå™¨"""
    
    def __init__(self):
        self.client = openai.AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
        self.model = settings.OPENAI_MODEL
        self.encoding = tiktoken.encoding_for_model("gpt-4")
        self.logger = logging.getLogger(__name__)
        self.monthly_usage = 0.0
        
        # ç²¾ç®€çš„ç³»ç»Ÿæç¤ºè¯
        self.system_prompt = """ä½ æ˜¯ä¸“ä¸šæŠ•èµ„åˆ†æå¸ˆï¼Œä»ç¤¾åª’æå–è‚¡ç¥¨ä¿¡å·ã€‚

**ä»»åŠ¡**: è¯†åˆ«ç¾è‚¡ä»£ç å’Œäº¤æ˜“åŠ¨ä½œï¼Œè¯„ä¼°ç½®ä¿¡åº¦ã€‚

**è¾“å‡ºJSONæ ¼å¼**:
{
  "original_text": "åŸæ–‡",
  "platform": "å¹³å°",
  "post_id": "ID", 
  "author": "ä½œè€…",
  "timestamp": "æ—¶é—´",
  "signals": [{
    "symbol": "AAPL",
    "action": "BUY",
    "confidence": 0.85,
    "target_price": 180.0,
    "reasoning": "ç†ç”±",
    "risk_level": "LOW"
  }],
  "sentiment": "BULLISH",
  "credibility_score": 0.75
}

**è§„åˆ™**: åªè¯†åˆ«æ˜ç¡®ä¿¡å·ï¼Œä¿å®ˆè¯„ä¼°ï¼Œæ— ä¿¡å·è¿”å›ç©ºæ•°ç»„ã€‚"""

    def estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—tokenæ•°é‡"""
        return len(self.encoding.encode(text))

    def check_budget(self) -> bool:
        """æ£€æŸ¥é¢„ç®—"""
        return self.monthly_usage < settings.OPENAI_MONTHLY_BUDGET

    @retry(stop=stop_after_attempt(2), wait=wait_exponential(multiplier=1, min=2, max=5))
    async def call_openai_api(self, messages: List[Dict[str, str]]) -> Dict[str, Any]:
        """ä¼˜åŒ–çš„APIè°ƒç”¨"""
        if not self.check_budget():
            raise Exception("Monthly budget exceeded")

        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=settings.OPENAI_MAX_TOKENS,
                temperature=0.1,
                response_format={"type": "json_object"}
            )
            
            # è®¡ç®—æˆæœ¬ï¼ˆGPT-4o-miniä»·æ ¼ï¼‰
            prompt_tokens = response.usage.prompt_tokens
            completion_tokens = response.usage.completion_tokens
            cost = prompt_tokens * 0.00015 + completion_tokens * 0.0006  # GPT-4o-miniå®šä»·
            self.monthly_usage += cost
            
            self.logger.info(f"OpenAI API: {response.usage.total_tokens} tokens, ${cost:.4f}")
            
            return {
                "content": response.choices[0].message.content,
                "usage": response.usage,
                "cost": cost
            }
            
        except Exception as e:
            self.logger.error(f"OpenAI API error: {e}")
            raise

    async def parse_post(self, text: str, platform: str, post_id: str, 
                        author: str = "unknown", timestamp: Optional[datetime] = None) -> ParsedContent:
        """è§£æå•ä¸ªå¸–å­"""
        if timestamp is None:
            timestamp = datetime.now()

        # æ£€æŸ¥æ–‡æœ¬é•¿åº¦ï¼Œæˆªæ–­ä»¥èŠ‚çœæˆæœ¬
        if self.estimate_tokens(text) > 1500:
            text = text[:1000]  # æ›´æ¿€è¿›çš„æˆªæ–­
            self.logger.warning(f"Text truncated for post {post_id}")

        # ç²¾ç®€çš„ç”¨æˆ·æç¤ºè¯
        user_prompt = f"""åˆ†æå¸–å­ï¼š
å¹³å°: {platform}
ä½œè€…: {author}
å†…å®¹: {text}

è¿”å›JSONæ ¼å¼åˆ†æç»“æœã€‚"""

        messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        try:
            response = await self.call_openai_api(messages)
            result_json = json.loads(response["content"])
            
            # ç¡®ä¿å­—æ®µå®Œæ•´
            result_json.update({
                "original_text": text,
                "platform": platform,
                "post_id": post_id,
                "author": author,
                "timestamp": timestamp.isoformat()
            })
            
            parsed_result = ParsedContent(**result_json)
            self.logger.info(f"Parsed post {post_id}: {len(parsed_result.signals)} signals")
            return parsed_result
            
        except (json.JSONDecodeError, Exception) as e:
            self.logger.error(f"Parse error for {post_id}: {e}")
            return self._create_default_result(text, platform, post_id, author, timestamp)

    def _create_default_result(self, text: str, platform: str, post_id: str, 
                              author: str, timestamp: datetime) -> ParsedContent:
        """åˆ›å»ºé»˜è®¤ç»“æœ"""
        return ParsedContent(
            original_text=text,
            platform=platform,
            post_id=post_id,
            author=author,
            timestamp=timestamp,
            signals=[],
            sentiment="NEUTRAL",
            credibility_score=0.0
        )

    async def batch_parse(self, posts: List[Dict[str, Any]], max_concurrent: int = 3) -> List[ParsedContent]:
        """æ‰¹é‡è§£æï¼Œæ§åˆ¶å¹¶å‘"""
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def parse_with_semaphore(post):
            async with semaphore:
                return await self.parse_post(
                    text=post["content"],
                    platform=post["platform"],
                    post_id=post["post_id"],
                    author=post.get("author", "unknown"),
                    timestamp=post.get("timestamp")
                )
        
        tasks = [parse_with_semaphore(post) for post in posts]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return [r for r in results if isinstance(r, ParsedContent)]

    def get_usage_stats(self) -> Dict[str, Any]:
        """è·å–ä½¿ç”¨ç»Ÿè®¡"""
        return {
            "monthly_usage": self.monthly_usage,
            "budget_limit": settings.OPENAI_MONTHLY_BUDGET,
            "remaining": settings.OPENAI_MONTHLY_BUDGET - self.monthly_usage,
            "usage_percent": (self.monthly_usage / settings.OPENAI_MONTHLY_BUDGET) * 100
        }
```

### 2.6 è´¨é‡é¢„ç­›é€‰å™¨ï¼ˆèŠ‚çœæˆæœ¬ï¼‰

```python
# src/utils/quality_filter.py
import re
from typing import Dict, List
import logging
from src.config.settings import settings

class QualityPrefilter:
    """è´¨é‡é¢„ç­›é€‰å™¨ - èŠ‚çœOpenAIæˆæœ¬"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # é«˜è´¨é‡å†…å®¹æŒ‡æ ‡
        self.quality_indicators = {
            'stock_symbols': [
                r'\$[A-Z]{1,5}\b',           # $AAPLæ ¼å¼
                r'\b[A-Z]{1,5}è‚¡ç¥¨\b',        # AAPLè‚¡ç¥¨
                r'\b[A-Z]{1,5}\s*å…¬å¸\b'      # AAPLå…¬å¸
            ],
            'trading_actions': [
                'ä¹°å…¥', 'å–å‡º', 'æŒæœ‰', 'æ¨è', 'å»ºä»“', 'å‡ä»“', 
                'buy', 'sell', 'hold', 'long', 'short', 'çœ‹æ¶¨', 'çœ‹è·Œ'
            ],
            'price_indicators': [
                r'\$\d+\.?\d*', r'ç›®æ ‡ä»·', r'ä»·æ ¼', r'ä¼°å€¼', 
                r'\d+ç¾å…ƒ', r'\d+åˆ€', r'ä»·ä½'
            ],
            'analysis_terms': [
                'åˆ†æ', 'ç ”æŠ¥', 'åŸºæœ¬é¢', 'æŠ€æœ¯é¢', 'è´¢æŠ¥', 'ä¸šç»©',
                'ç›ˆåˆ©', 'æ”¶å…¥', 'å¢é•¿', 'å¸‚åœº', 'è¡Œä¸š', 'ç«äº‰'
            ]
        }
        
        # ä½è´¨é‡å†…å®¹è¿‡æ»¤
        self.noise_keywords = [
            'æ—©å®‰', 'æ™šå®‰', 'åƒé¥­', 'ç¡è§‰', 'å¤©æ°”', 'å¿ƒæƒ…',
            'è‡ªæ‹', 'ç¾é£Ÿ', 'æ—…æ¸¸', 'è´­ç‰©', 'åŒ–å¦†', 'ç©¿æ­'
        ]
    
    def calculate_quality_score(self, text: str, platform: str, author: str) -> float:
        """è®¡ç®—å†…å®¹è´¨é‡åˆ†æ•°"""
        score = 0.0
        text_lower = text.lower()
        
        # 1. è‚¡ç¥¨ç¬¦å·æ£€æŸ¥ (35%)
        stock_found = any(re.search(pattern, text) for pattern in self.quality_indicators['stock_symbols'])
        if stock_found:
            score += 0.35
        
        # 2. äº¤æ˜“åŠ¨ä½œæ£€æŸ¥ (30%)
        action_found = any(keyword in text_lower for keyword in self.quality_indicators['trading_actions'])
        if action_found:
            score += 0.30
        
        # 3. ä»·æ ¼ä¿¡æ¯æ£€æŸ¥ (20%)
        price_found = any(re.search(pattern, text) for pattern in self.quality_indicators['price_indicators'])
        if price_found:
            score += 0.20
        
        # 4. åˆ†ææ·±åº¦æ£€æŸ¥ (10%)
        analysis_found = any(keyword in text_lower for keyword in self.quality_indicators['analysis_terms'])
        if analysis_found:
            score += 0.10
        
        # 5. å™ªéŸ³å†…å®¹æƒ©ç½š
        noise_found = any(keyword in text_lower for keyword in self.noise_keywords)
        if noise_found:
            score *= 0.5  # å™ªéŸ³å†…å®¹åˆ†æ•°å‡åŠ
        
        # 6. é•¿åº¦åŠ æƒ
        if len(text) < 50:
            score *= 0.7  # å†…å®¹å¤ªçŸ­
        elif len(text) > 500:
            score *= 1.1  # å†…å®¹è¯¦ç»†
        
        # 7. å¹³å°æƒé‡
        platform_weights = {
            'xiaohongshu': 0.9,
            'twitter': 1.0,
            'weibo': 0.8
        }
        score *= platform_weights.get(platform, 0.5)
        
        return min(score, 1.0)
    
    def should_parse_with_ai(self, text: str, platform: str, author: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦AIè§£æ"""
        quality_score = self.calculate_quality_score(text, platform, author)
        
        # è®°å½•ç­›é€‰ç»“æœ
        self.logger.info(f"Quality score for {platform} post: {quality_score:.2f}")
        
        # åŠ¨æ€é˜ˆå€¼ï¼Œç¡®ä¿è‡³å°‘æœ‰ä¸€äº›å†…å®¹ä¼šè¢«è§£æ
        threshold = settings.QUALITY_THRESHOLD
        
        return quality_score >= threshold
    
    def get_filter_stats(self) -> Dict[str, int]:
        """è·å–ç­›é€‰ç»Ÿè®¡"""
        # TODO: ä»æ•°æ®åº“ç»Ÿè®¡ç­›é€‰æ•ˆæœ
        return {
            "total_posts": 0,
            "filtered_posts": 0,
            "ai_parsed_posts": 0,
            "cost_saved_percent": 0
        }

# å…¨å±€è´¨é‡ç­›é€‰å™¨å®ä¾‹
quality_filter = QualityPrefilter()
```

### 2.7 ç¤¾åª’ç›‘æ§å™¨ï¼ˆè½»é‡çº§ï¼‰

```python
# src/monitors/social_monitor.py
import asyncio
import aiohttp
import time
import re
from typing import List, Dict, Optional
import logging
from datetime import datetime, timedelta
from src.config.settings import settings
from src.utils.quality_filter import quality_filter

class LightweightSocialMonitor:
    """è½»é‡çº§ç¤¾åª’ç›‘æ§å™¨ - é€‚é…å…è´¹ç‰ˆèµ„æºé™åˆ¶"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.session = None
        self.last_check_times = {}
        
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()
    
    async def monitor_xiaohongshu_lite(self, blogger_id: str) -> List[Dict]:
        """è½»é‡çº§å°çº¢ä¹¦ç›‘æ§ - å‡å°‘èµ„æºæ¶ˆè€—"""
        posts = []
        
        try:
            # ç®€åŒ–çš„APIè°ƒç”¨ï¼Œå‡å°‘æ•°æ®ä¼ è¾“
            url = f"https://www.xiaohongshu.com/user/profile/{blogger_id}"
            
            async with self.session.get(url) as response:
                if response.status != 200:
                    self.logger.warning(f"Failed to fetch {url}: {response.status}")
                    return posts
                
                html = await response.text()
                
                # ç®€åŒ–çš„å†…å®¹æå–
                post_ids = re.findall(r'/explore/([a-zA-Z0-9]{20,})', html)
                
                # åªå¤„ç†å‰5ä¸ªæœ€æ–°å¸–å­ï¼Œå‡å°‘èµ„æºæ¶ˆè€—
                for post_id in post_ids[:5]:
                    if self.is_new_post("xiaohongshu", post_id):
                        post_content = await self.extract_post_content_lite(post_id)
                        if post_content:
                            # è´¨é‡é¢„ç­›é€‰
                            if quality_filter.should_parse_with_ai(
                                post_content, "xiaohongshu", blogger_id
                            ):
                                posts.append({
                                    "platform": "xiaohongshu",
                                    "post_id": post_id,
                                    "content": post_content,
                                    "author": blogger_id,
                                    "timestamp": datetime.now(),
                                    "quality_score": quality_filter.calculate_quality_score(
                                        post_content, "xiaohongshu", blogger_id
                                    )
                                })
                
        except Exception as e:
            self.logger.error(f"Error monitoring xiaohongshu {blogger_id}: {e}")
        
        return posts
    
    async def extract_post_content_lite(self, post_id: str) -> Optional[str]:
        """è½»é‡çº§å†…å®¹æå–"""
        try:
            url = f"https://www.xiaohongshu.com/explore/{post_id}"
            
            async with self.session.get(url) as response:
                if response.status != 200:
                    return None
                
                html = await response.text()
                
                # ç®€åŒ–çš„å†…å®¹æå–æ­£åˆ™
                content_patterns = [
                    r'"desc":"([^"]+)"',
                    r'"title":"([^"]+)"',
                    r'content["\']:\s*["\']([^"\']+)["\']'
                ]
                
                for pattern in content_patterns:
                    matches = re.findall(pattern, html)
                    if matches:
                        # æ¸…ç†å’Œè§£ç å†…å®¹
                        content = matches[0].replace('\\n', ' ').replace('\\', '')
                        if len(content) > 20:  # ç¡®ä¿å†…å®¹æœ‰æ„ä¹‰
                            return content[:500]  # é™åˆ¶é•¿åº¦èŠ‚çœtoken
                
        except Exception as e:
            self.logger.warning(f"Failed to extract content for {post_id}: {e}")
        
        return None
    
    def is_new_post(self, platform: str, post_id: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæ–°å¸–å­ - ç®€åŒ–ç‰ˆ"""
        cache_key = f"{platform}_{post_id}"
        
        # ç®€å•çš„å†…å­˜ç¼“å­˜ï¼Œé¿å…æ•°æ®åº“æŸ¥è¯¢
        if not hasattr(self, '_seen_posts'):
            self._seen_posts = set()
        
        if cache_key in self._seen_posts:
            return False
        
        self._seen_posts.add(cache_key)
        
        # é™åˆ¶ç¼“å­˜å¤§å°ï¼Œé¿å…å†…å­˜æº¢å‡º
        if len(self._seen_posts) > 1000:
            # æ¸…ç†ä¸€åŠæ—§è®°å½•
            old_posts = list(self._seen_posts)[:500]
            for post in old_posts:
                self._seen_posts.discard(post)
        
        return True
    
    async def monitor_all_sources(self) -> List[Dict]:
        """ç›‘æ§æ‰€æœ‰é…ç½®çš„ä¿¡æ¯æº"""
        all_posts = []
        
        # å¹¶å‘ç›‘æ§ï¼Œä½†é™åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(2)  # é™åˆ¶å¹¶å‘æ•°
        
        async def monitor_blogger(blogger_id):
            async with semaphore:
                try:
                    posts = await self.monitor_xiaohongshu_lite(blogger_id)
                    return posts
                except Exception as e:
                    self.logger.error(f"Error monitoring {blogger_id}: {e}")
                    return []
        
        # åˆ›å»ºä»»åŠ¡
        tasks = [monitor_blogger(blogger_id) for blogger_id in settings.BLOGGER_IDS[:3]]  # é™åˆ¶åšä¸»æ•°é‡
        
        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æ”¶é›†ç»“æœ
        for result in results:
            if isinstance(result, list):
                all_posts.extend(result)
        
        self.logger.info(f"Found {len(all_posts)} qualified posts from {len(settings.BLOGGER_IDS)} sources")
        return all_posts

# ä½¿ç”¨ç¤ºä¾‹
async def test_monitor():
    """æµ‹è¯•ç›‘æ§å™¨"""
    async with LightweightSocialMonitor() as monitor:
        posts = await monitor.monitor_all_sources()
        for post in posts:
            print(f"Found post: {post['post_id']} - Score: {post['quality_score']:.2f}")

if __name__ == "__main__":
    asyncio.run(test_monitor())
```

### 2.8 é‚®ä»¶æœåŠ¡ï¼ˆGmailä¼˜åŒ–ï¼‰

```python
# src/utils/email_service.py
import asyncio
import aiosmtplib
from email.mime.text import MimeText
from email.mime.multipart import MimeMultipart
import json
from typing import List, Dict, Any
from datetime import datetime
import logging
from src.config.settings import settings

class GmailEmailService:
    """Gmailä¼˜åŒ–çš„é‚®ä»¶æœåŠ¡"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.daily_sent = 0  # è·Ÿè¸ªæ¯æ—¥å‘é€é‡
        self.daily_limit = 450  # Gmailå…è´¹ç‰ˆé™åˆ¶500/å¤©ï¼Œä¿ç•™ç¼“å†²
        
    async def send_email(self, subject: str, body: str, to_email: str = None, 
                        html_body: str = None) -> bool:
        """å‘é€é‚®ä»¶"""
        if self.daily_sent >= self.daily_limit:
            self.logger.warning("Daily email limit reached")
            return False
            
        if not to_email:
            to_email = settings.ALERT_EMAIL
            
        try:
            # åˆ›å»ºé‚®ä»¶
            msg = MimeMultipart('alternative')
            msg['From'] = settings.SMTP_USERNAME
            msg['To'] = to_email
            msg['Subject'] = subject
            
            # æ·»åŠ æ–‡æœ¬å†…å®¹
            msg.attach(MimeText(body, 'plain', 'utf-8'))
            
            # æ·»åŠ HTMLå†…å®¹
            if html_body:
                msg.attach(MimeText(html_body, 'html', 'utf-8'))
            
            # å‘é€é‚®ä»¶
            await aiosmtplib.send(
                msg,
                hostname=settings.SMTP_HOST,
                port=settings.SMTP_PORT,
                start_tls=True,
                username=settings.SMTP_USERNAME,
                password=settings.SMTP_PASSWORD,
                timeout=30
            )
            
            self.daily_sent += 1
            self.logger.info(f"Email sent successfully to {to_email} ({self.daily_sent}/{self.daily_limit})")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to send email: {e}")
            return False

    async def send_low_confidence_alert(self, signals: List[Dict], post_info: Dict) -> bool:
        """å‘é€ä½ç½®ä¿¡åº¦ä¿¡å·é€šçŸ¥ - ç²¾ç®€ç‰ˆ"""
        
        subject = f"ğŸ” ä½ç½®ä¿¡åº¦æŠ•èµ„ä¿¡å· - {len(signals)}ä¸ªä¿¡å·éœ€è¦å®¡æ ¸"
        
        # ç²¾ç®€çš„æ–‡æœ¬ç‰ˆæœ¬
        body_lines = [
            f"æ£€æµ‹åˆ° {len(signals)} ä¸ªä½ç½®ä¿¡åº¦æŠ•èµ„ä¿¡å·ï¼Œè¯·æ‰‹åŠ¨å®¡æ ¸ï¼š",
            "",
            f"ğŸ“± æ¥æº: {post_info['platform']} - {post_info['author']}",
            f"ğŸ•’ æ—¶é—´: {post_info['timestamp']}",
            "",
            f"ğŸ“ åŸæ–‡: {post_info['content'][:150]}{'...' if len(post_info['content']) > 150 else ''}",
            "",
            "ğŸ“Š è§£æä¿¡å·:"
        ]
        
        for i, signal in enumerate(signals[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ªä¿¡å·
            body_lines.extend([
                f"{i}. {signal['symbol']} - {signal['action']}",
                f"   ç½®ä¿¡åº¦: {signal['confidence']:.2f} | ç›®æ ‡: ${signal.get('target_price', 'N/A')}",
                f"   ç†ç”±: {signal['reasoning'][:80]}{'...' if len(signal['reasoning']) > 80 else ''}",
                ""
            ])
        
        if len(signals) > 3:
            body_lines.append(f"...è¿˜æœ‰ {len(signals) - 3} ä¸ªä¿¡å·")
        
        body_lines.extend([
            "",
            "ğŸ’¡ å»ºè®®: ä»”ç»†åˆ†æåæ‰‹åŠ¨æ‰§è¡Œäº¤æ˜“",
            "âš ï¸  æŠ•èµ„æœ‰é£é™©ï¼Œä»…ä¾›å‚è€ƒï¼"
        ])
        
        body = "\n".join(body_lines)
        
        # ç®€åŒ–çš„HTMLç‰ˆæœ¬
        html_body = self._build_simple_html_alert(signals, post_info)
        
        return await self.send_email(subject, body, html_body=html_body)

    def _build_simple_html_alert(self, signals: List[Dict], post_info: Dict) -> str:
        """æ„å»ºç®€åŒ–çš„HTMLé‚®ä»¶"""
        
        signals_html = ""
        for signal in signals[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
            conf_color = "#28a745" if signal['confidence'] > 0.7 else "#ffc107" if signal['confidence'] > 0.5 else "#dc3545"
            
            signals_html += f"""
            <div style="border: 1px solid #ddd; padding: 10px; margin: 10px 0; border-radius: 5px;">
                <h4>{signal['symbol']} - {signal['action']}</h4>
                <p><strong>ç½®ä¿¡åº¦:</strong> <span style="color: {conf_color}">{signal['confidence']:.2f}</span></p>
                <p><strong>ç›®æ ‡ä»·:</strong> ${signal.get('target_price', 'N/A')}</p>
                <p><strong>ç†ç”±:</strong> {signal['reasoning'][:100]}{'...' if len(signal['reasoning']) > 100 else ''}</p>
            </div>
            """
        
        html = f"""
        <!DOCTYPE html>
        <html>
        <head><meta charset="utf-8"></head>
        <body style="font-family: Arial, sans-serif; max-width: 600px;">
            <h2>ğŸ” ä½ç½®ä¿¡åº¦æŠ•èµ„ä¿¡å·é€šçŸ¥</h2>
            
            <div style="background: #f8f9fa; padding: 15px; border-radius: 5px; margin-bottom: 20px;">
                <p><strong>å¹³å°:</strong> {post_info['platform']}</p>
                <p><strong>ä½œè€…:</strong> {post_info['author']}</p>
                <p><strong>æ—¶é—´:</strong> {post_info['timestamp']}</p>
            </div>
            
            <div style="background: #f8f9fa; padding: 15px; border-radius: 5px; margin-bottom: 20px;">
                <h3>ğŸ“ åŸæ–‡å†…å®¹</h3>
                <p>{post_info['content'][:200]}{'...' if len(post_info['content']) > 200 else ''}</p>
            </div>
            
            <h3>ğŸ“Š è§£æä¿¡å· ({len(signals)}ä¸ª)</h3>
            {signals_html}
            
            {f'<p><em>è¿˜æœ‰ {len(signals) - 3} ä¸ªä¿¡å·æœªæ˜¾ç¤º</em></p>' if len(signals) > 3 else ''}
            
            <div style="background: #fff3cd; padding: 15px; border-radius: 5px; margin-top: 20px;">
                <p><strong>âš ï¸ æé†’:</strong> è¿™æ˜¯AIè‡ªåŠ¨åˆ†æç»“æœï¼Œä»…ä¾›å‚è€ƒã€‚è¯·æ ¹æ®è‡ªå·±çš„åˆ¤æ–­å’Œé£é™©æ‰¿å—èƒ½åŠ›åšå‡ºæŠ•èµ„å†³ç­–ã€‚</p>
            </div>
        </body>
        </html>
        """
        
        return html

    async def send_daily_summary(self, summary_data: Dict) -> bool:
        """å‘é€æ¯æ—¥æ€»ç»“ - ç²¾ç®€ç‰ˆ"""
        
        subject = f"ğŸ“Š AIäº¤æ˜“ç³»ç»Ÿæ—¥æŠ¥ - {datetime.now().strftime('%Y-%m-%d')}"
        
        body = f"""
ğŸ“Š AIäº¤æ˜“ç³»ç»Ÿæ¯æ—¥æ€»ç»“

ğŸ“± ç›‘æ§ç»Ÿè®¡:
- æ£€æŸ¥å¸–å­: {summary_data.get('total_posts', 0)}
- é€šè¿‡ç­›é€‰: {summary_data.get('qualified_posts', 0)}
- AIè§£æ: {summary_data.get('parsed_posts', 0)}

ğŸ¤– AIä½¿ç”¨:
- APIè°ƒç”¨: {summary_data.get('ai_requests', 0)}æ¬¡
- å½“æ—¥æˆæœ¬: ${summary_data.get('ai_cost', 0):.2f}
- é¢„ç®—ä½¿ç”¨: {summary_data.get('budget_used_percent', 0):.1f}%

ğŸ“Š ä¿¡å·ç»Ÿè®¡:
- æ€»ä¿¡å·: {summary_data.get('total_signals', 0)}
- é«˜ç½®ä¿¡åº¦: {summary_data.get('high_conf_signals', 0)}
- é‚®ä»¶é€šçŸ¥: {summary_data.get('email_alerts', 0)}

ğŸ”§ ç³»ç»ŸçŠ¶æ€: {summary_data.get('system_status', 'RUNNING')}

---
å‘é€æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
        """
        
        return await self.send_email(subject, body)

    async def send_system_alert(self, message: str, alert_type: str = "INFO") -> bool:
        """å‘é€ç³»ç»Ÿå‘Šè­¦"""
        
        icons = {"INFO": "â„¹ï¸", "WARNING": "âš ï¸", "ERROR": "âŒ", "SUCCESS": "âœ…"}
        subject = f"{icons.get(alert_type, 'ğŸ“¢')} ç³»ç»Ÿé€šçŸ¥ - {alert_type}"
        
        body = f"""
{icons.get(alert_type, 'ğŸ“¢')} ç³»ç»Ÿé€šçŸ¥

ç±»å‹: {alert_type}
æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

è¯¦æƒ…:
{message}

---
AIäº¤æ˜“ç³»ç»Ÿè‡ªåŠ¨å‘é€
        """
        
        return await self.send_email(subject, body)

# å…¨å±€é‚®ä»¶æœåŠ¡å®ä¾‹
email_service = GmailEmailService()
```

---

## ğŸš€ ç¬¬ä¸‰é˜¶æ®µï¼šä¸»äº¤æ˜“å¼•æ“ï¼ˆç¬¬8-10å¤©ï¼‰

### 3.1 æ ¸å¿ƒäº¤æ˜“å¼•æ“ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆï¼‰

```python
# src/trading_engine.py
import asyncio
import logging
import gc
from typing import List, Dict, Any
from datetime import datetime, timedelta
from src.parsers.openai_parser import OptimizedOpenAIParser
from src.monitors.social_monitor import LightweightSocialMonitor
from src.utils.email_service import email_service
from src.models.database import AsyncDatabaseManager, SessionLocal, AISignal, BloggerPost, Trade, SystemMetrics
from src.config.settings import settings
import json
import psutil

class MemoryOptimizedTradingEngine:
    """å†…å­˜ä¼˜åŒ–çš„äº¤æ˜“å¼•æ“ - é€‚é…å…è´¹ç‰ˆé™åˆ¶"""
    
    def __init__(self):
        self.ai_parser = OptimizedOpenAIParser()
        self.db_manager = AsyncDatabaseManager()
        self.logger = logging.getLogger(__name__)
        
        # ç»Ÿè®¡æ•°æ®
        self.daily_stats = {
            'total_posts': 0,
            'qualified_posts': 0,
            'parsed_posts': 0,
            'ai_requests': 0,
            'ai_cost': 0.0,
            'total_signals': 0,
            'high_conf_signals': 0,
            'email_alerts': 0
        }
        
        # å†…å­˜ç›‘æ§
        self.memory_threshold = settings.MAX_MEMORY_MB * 1024 * 1024  # è½¬æ¢ä¸ºå­—èŠ‚
    
    def check_memory_usage(self) -> Dict[str, Any]:
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        process = psutil.Process()
        memory_info = process.memory_info()
        
        return {
            "memory_mb": memory_info.rss / 1024 / 1024,
            "memory_percent": process.memory_percent(),
            "memory_available_mb": (self.memory_threshold - memory_info.rss) / 1024 / 1024,
            "memory_critical": memory_info.rss > self.memory_threshold * 0.9
        }
    
    def cleanup_memory(self):
        """æ¸…ç†å†…å­˜"""
        gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
        
        # æ¸…ç†è§£æå™¨ç¼“å­˜
        if hasattr(self.ai_parser, '_seen_posts'):
            if len(self.ai_parser._seen_posts) > 500:
                self.ai_parser._seen_posts.clear()
        
        self.logger.info("Memory cleanup completed")

    async def run_monitoring_cycle(self) -> bool:
        """æ‰§è¡Œä¸€è½®ç›‘æ§å‘¨æœŸ"""
        
        self.logger.info("Starting optimized monitoring cycle...")
        
        try:
            # å†…å­˜æ£€æŸ¥
            memory_info = self.check_memory_usage()
            if memory_info["memory_critical"]:
                self.cleanup_memory()
                self.logger.warning(f"Memory critical: {memory_info['memory_mb']:.1f}MB")
            
            # 1. ç›‘æ§ç¤¾åª’ï¼ˆä½¿ç”¨å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼‰
            async with LightweightSocialMonitor() as monitor:
                new_posts = await monitor.monitor_all_sources()
            
            self.daily_stats['total_posts'] += len(new_posts)
            
            if not new_posts:
                self.logger.info("No new qualified posts found")
                return True
            
            self.daily_stats['qualified_posts'] += len(new_posts)
            
            # 2. æ‰¹é‡AIè§£æï¼ˆæ§åˆ¶å¹¶å‘ï¼‰
            parsed_results = await self.ai_parser.batch_parse(new_posts, max_concurrent=2)
            self.daily_stats['parsed_posts'] += len(parsed_results)
            self.daily_stats['ai_requests'] += len(new_posts)
            
            # 3. å¤„ç†è§£æç»“æœ
            for result in parsed_results:
                await self.process_parsed_result(result)
            
            # 4. æ›´æ–°AIä½¿ç”¨ç»Ÿè®¡
            ai_stats = self.ai_parser.get_usage_stats()
            self.daily_stats['ai_cost'] = ai_stats['monthly_usage']
            
            # 5. å†…å­˜æ¸…ç†
            self.cleanup_memory()
            
            self.logger.info(f"Cycle completed: {len(new_posts)} posts, "
                           f"{self.daily_stats['high_conf_signals']} high-conf signals, "
                           f"Memory: {memory_info['memory_mb']:.1f}MB")
            
            return True
            
        except Exception as e:
            self.logger.error(f"Error in monitoring cycle: {e}")
            await email_service.send_system_alert(f"ç›‘æ§å‘¨æœŸé”™è¯¯: {e}", "ERROR")
            return False
    
    async def process_parsed_result(self, result):
        """å¤„ç†AIè§£æç»“æœ"""
        try:
            # ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆä½¿ç”¨å¼‚æ­¥ï¼‰
            await self.save_parsed_result_to_db(result)
            
            # ç»Ÿè®¡ä¿¡å·
            self.daily_stats['total_signals'] += len(result.signals)
            
            # åˆ†ç±»å¤„ç†ä¿¡å·
            high_conf_signals = []
            low_conf_signals = []
            
            for signal in result.signals:
                if (signal.confidence >= settings.HIGH_CONFIDENCE_THRESHOLD and 
                    result.credibility_score >= settings.MIN_CREDIBILITY_SCORE):
                    high_conf_signals.append(signal)
                elif signal.confidence >= settings.MEDIUM_CONFIDENCE_THRESHOLD:
                    low_conf_signals.append(signal)
            
            # å¤„ç†é«˜ç½®ä¿¡åº¦ä¿¡å· - è‡ªåŠ¨æ‰§è¡Œï¼ˆæ¨¡æ‹Ÿï¼‰
            if high_conf_signals:
                await self.execute_high_confidence_signals(high_conf_signals, result)
                self.daily_stats['high_conf_signals'] += len(high_conf_signals)
            
            # å¤„ç†ä½ç½®ä¿¡åº¦ä¿¡å· - é‚®ä»¶é€šçŸ¥
            if low_conf_signals:
                await self.notify_low_confidence_signals(low_conf_signals, result)
                self.daily_stats['email_alerts'] += 1
            
        except Exception as e:
            self.logger.error(f"Error processing parsed result: {e}")
    
    async def save_parsed_result_to_db(self, result):
        """ä¿å­˜è§£æç»“æœåˆ°æ•°æ®åº“"""
        try:
            # ä½¿ç”¨å¼‚æ­¥æ•°æ®åº“è¿æ¥
            insert_post_query = """
                INSERT INTO blogger_posts (platform, post_id, author, content, quality_score, 
                                         ai_parsed, extracted_signals, ai_analysis, processed)
                VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
                ON CONFLICT (post_id) DO NOTHING
            """
            
            await self.db_manager.execute_query(
                insert_post_query,
                result.platform, result.post_id, result.author, result.original_text,
                0.8, True, json.dumps([s.dict() for s in result.signals]),
                json.dumps({"sentiment": result.sentiment, "credibility_score": result.credibility_score}),
                True
            )
            
            # ä¿å­˜ä¿¡å·
            for signal in result.signals:
                insert_signal_query = """
                    INSERT INTO ai_signals (post_id, symbol, action, confidence, 
                                          credibility_score, target_price, reasoning, 
                                          risk_level, status)
                    VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
                """
                
                await self.db_manager.execute_query(
                    insert_signal_query,
                    result.post_id, signal.symbol, signal.action, signal.confidence,
                    result.credibility_score, signal.target_price, signal.reasoning,
                    signal.risk_level, "PENDING"
                )
            
            self.logger.info(f"Saved post {result.post_id} with {len(result.signals)} signals")
            
        except Exception as e:
            self.logger.error(f"Failed to save parsed result: {e}")
    
    async def execute_high_confidence_signals(self, signals: List, context):
        """æ‰§è¡Œé«˜ç½®ä¿¡åº¦ä¿¡å·ï¼ˆæ¨¡æ‹Ÿç‰ˆæœ¬ï¼‰"""
        
        for signal in signals:
            try:
                # åŸºç¡€é£é™©æ£€æŸ¥
                if not await self.basic_risk_check(signal):
                    continue
                
                # æ¨¡æ‹Ÿäº¤æ˜“ä¿¡å·ç”Ÿæˆ
                trading_signal = {
                    "symbol": signal.symbol,
                    "action": signal.action,
                    "confidence": signal.confidence,
                    "target_price": signal.target_price,
                    "reasoning": signal.reasoning,
                    "timestamp": datetime.now().isoformat()
                }
                
                # è®°å½•åˆ°æ•°æ®åº“
                await self.record_simulated_trade(signal, context, trading_signal)
                
                # å‘é€æˆåŠŸé€šçŸ¥
                await email_service.send_system_alert(
                    f"æ¨¡æ‹Ÿæ‰§è¡Œäº¤æ˜“: {signal.symbol} {signal.action} (ç½®ä¿¡åº¦: {signal.confidence:.2f})",
                    "SUCCESS"
                )
                
                self.logger.info(f"Simulated trade: {signal.symbol} {signal.action}")
                
            except Exception as e:
                self.logger.error(f"Failed to execute signal {signal.symbol}: {e}")
    
    async def basic_risk_check(self, signal) -> bool:
        """åŸºç¡€é£é™©æ£€æŸ¥"""
        
        # æ£€æŸ¥æ¯æ—¥äº¤æ˜“æ¬¡æ•°é™åˆ¶
        if self.daily_stats['high_conf_signals'] >= settings.MAX_DAILY_TRADES:
            self.logger.warning("Daily trade limit reached")
            return False
        
        # æ£€æŸ¥é£é™©ç­‰çº§
        if signal.risk_level == "HIGH":
            self.logger.warning(f"High risk signal skipped: {signal.symbol}")
            return False
        
        return True
    
    async def record_simulated_trade(self, signal, context, trading_signal):
        """è®°å½•æ¨¡æ‹Ÿäº¤æ˜“"""
        try:
            insert_trade_query = """
                INSERT INTO trades (symbol, action, quantity, entry_price, 
                                  tradingview_alert_id, status)
                VALUES ($1, $2, $3, $4, $5, $6)
            """
            
            await self.db_manager.execute_query(
                insert_trade_query,
                signal.symbol, signal.action, 100,  # æ¨¡æ‹Ÿæ•°é‡
                signal.target_price or 0.0,
                f"sim_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{signal.symbol}",
                "SIMULATED"
            )
            
        except Exception as e:
            self.logger.error(f"Failed to record trade: {e}")
    
    async def notify_low_confidence_signals(self, signals: List, context):
        """é€šçŸ¥ä½ç½®ä¿¡åº¦ä¿¡å·"""
        
        try:
            # æ„å»ºé‚®ä»¶æ•°æ®
            post_info = {
                "platform": context.platform,
                "author": context.author,
                "timestamp": context.timestamp.strftime('%Y-%m-%d %H:%M:%S'),
                "content": context.original_text
            }
            
            signals_data = [signal.dict() for signal in signals]
            
            # å‘é€é‚®ä»¶é€šçŸ¥
            success = await email_service.send_low_confidence_alert(signals_data, post_info)
            
            if success:
                # æ›´æ–°ä¿¡å·çŠ¶æ€
                for signal in signals:
                    update_query = """
                        UPDATE ai_signals 
                        SET status = 'NOTIFIED', execution_method = 'EMAIL'
                        WHERE post_id = $1 AND symbol = $2
                    """
                    await self.db_manager.execute_query(update_query, context.post_id, signal.symbol)
                
                self.logger.info(f"Sent email notification for {len(signals)} low-confidence signals")
            
        except Exception as e:
            self.logger.error(f"Failed to send notification: {e}")
    
    async def send_daily_summary(self):
        """å‘é€æ¯æ—¥æ€»ç»“"""
        
        try:
            summary_data = self.daily_stats.copy()
            
            # è®¡ç®—é¢„ç®—ä½¿ç”¨ç™¾åˆ†æ¯”
            ai_stats = self.ai_parser.get_usage_stats()
            summary_data['budget_used_percent'] = ai_stats['usage_percent']
            summary_data['system_status'] = "RUNNING"
            
            await email_service.send_daily_summary(summary_data)
            
            # é‡ç½®æ¯æ—¥ç»Ÿè®¡
            self.daily_stats = {key: 0 if isinstance(value, (int, float)) else value 
                               for key, value in self.daily_stats.items()}
            
        except Exception as e:
            self.logger.error(f"Failed to send daily summary: {e}")
    
    async def run_scheduler(self):
        """è¿è¡Œè°ƒåº¦å™¨"""
        self.logger.info("Free tier trading engine started")
        
        await self.db_manager.connect()
        last_daily_summary = datetime.now().date()
        cycle_count = 0
        
        try:
            while True:
                cycle_count += 1
                
                # æ‰§è¡Œç›‘æ§å‘¨æœŸ
                success = await self.run_monitoring_cycle()
                
                if not success:
                    # å¦‚æœå‘¨æœŸå¤±è´¥ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´
                    await asyncio.sleep(300)  # 5åˆ†é’Ÿ
                    continue
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦å‘é€æ¯æ—¥æ€»ç»“
                current_date = datetime.now().date()
                if current_date > last_daily_summary:
                    await self.send_daily_summary()
                    last_daily_summary = current_date
                
                # æ¯10ä¸ªå‘¨æœŸè¿›è¡Œå†…å­˜æ¸…ç†
                if cycle_count % 10 == 0:
                    self.cleanup_memory()
                    memory_info = self.check_memory_usage()
                    self.logger.info(f"Memory status: {memory_info['memory_mb']:.1f}MB")
                
                # ç­‰å¾…ä¸‹ä¸€ä¸ªå‘¨æœŸ
                await asyncio.sleep(settings.CHECK_INTERVAL_MINUTES * 60)
                
        except KeyboardInterrupt:
            self.logger.info("Scheduler stopped by user")
        except Exception as e:
            self.logger.error(f"Scheduler error: {e}")
            await email_service.send_system_alert(f"è°ƒåº¦å™¨ä¸¥é‡é”™è¯¯: {e}", "ERROR")
        finally:
            await self.db_manager.disconnect()

# å…¨å±€äº¤æ˜“å¼•æ“å®ä¾‹
trading_engine = MemoryOptimizedTradingEngine()
```

### 3.2 FastAPIä¸»åº”ç”¨

```python
# src/main.py
import asyncio
import logging
import sys
from datetime import datetime
from fastapi import FastAPI, BackgroundTasks, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import uvicorn
import signal
import gc
import psutil

from src.trading_engine import trading_engine
from src.models.database import SessionLocal, AISignal, Trade, SystemMetrics
from src.config.settings import settings

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)  # Railway.appæ—¥å¿—è¾“å‡º
    ]
)

logger = logging.getLogger(__name__)

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="Free AI Trading System",
    description="OpenAI + Supabase + Railway.app å…è´¹AIäº¤æ˜“ç³»ç»Ÿ",
    version="1.0.0"
)

# CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€å˜é‡
scheduler_task = None

@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨äº‹ä»¶"""
    global scheduler_task
    
    logger.info("ğŸš€ Starting Free AI Trading System...")
    logger.info(f"ğŸ“Š Configuration: High confidence = {settings.HIGH_CONFIDENCE_THRESHOLD}")
    logger.info(f"ğŸ‘¥ Monitoring {len(settings.BLOGGER_IDS)} bloggers")
    logger.info(f"ğŸ’° OpenAI budget: ${settings.OPENAI_MONTHLY_BUDGET}/month")
    
    # å¯åŠ¨åå°è°ƒåº¦å™¨
    scheduler_task = asyncio.create_task(trading_engine.run_scheduler())
    
    # å‘é€å¯åŠ¨é€šçŸ¥
    try:
        from src.utils.email_service import email_service
        await email_service.send_system_alert(
            f"ğŸš€ å…è´¹AIäº¤æ˜“ç³»ç»Ÿå·²å¯åŠ¨\n\n"
            f"ğŸ“Š é…ç½®ä¿¡æ¯:\n"
            f"- é«˜ç½®ä¿¡åº¦é˜ˆå€¼: {settings.HIGH_CONFIDENCE_THRESHOLD}\n"
            f"- ç›‘æ§åšä¸»æ•°é‡: {len(settings.BLOGGER_IDS)}\n"
            f"- OpenAIé¢„ç®—: ${settings.OPENAI_MONTHLY_BUDGET}/æœˆ\n"
            f"- æ£€æŸ¥é—´éš”: {settings.CHECK_INTERVAL_MINUTES}åˆ†é’Ÿ\n\n"
            f"ç³»ç»Ÿå°†è‡ªåŠ¨ç›‘æ§ç¤¾åª’ä¿¡å·å¹¶å‘é€é‚®ä»¶é€šçŸ¥ï¼",
            "SUCCESS"
        )
    except Exception as e:
        logger.error(f"Failed to send startup notification: {e}")

@app.on_event("shutdown")
async def shutdown_event():
    """å…³é—­äº‹ä»¶"""
    global scheduler_task
    
    logger.info("ğŸ›‘ Shutting down AI Trading System...")
    
    if scheduler_task:
        scheduler_task.cancel()
        try:
            await scheduler_task
        except asyncio.CancelledError:
            pass
    
    # å‘é€å…³é—­é€šçŸ¥
    try:
        from src.utils.email_service import email_service
        await email_service.send_system_alert("ğŸ›‘ AIäº¤æ˜“ç³»ç»Ÿå·²åœæ­¢", "INFO")
    except Exception as e:
        logger.error(f"Failed to send shutdown notification: {e}")

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "ğŸš€ Free AI Trading System is running!",
        "version": "1.0.0",
        "status": "healthy",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    try:
        # æ£€æŸ¥å†…å­˜ä½¿ç”¨
        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024
        
        # æ£€æŸ¥æ•°æ®åº“è¿æ¥
        db = SessionLocal()
        db.execute("SELECT 1")
        db.close()
        
        # æ£€æŸ¥AIè§£æå™¨çŠ¶æ€
        ai_stats = trading_engine.ai_parser.get_usage_stats()
        
        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "memory_mb": round(memory_mb, 1),
            "memory_limit_mb": settings.MAX_MEMORY_MB,
            "openai_budget_used": f"{ai_stats['usage_percent']:.1f}%",
            "database": "connected"
        }
        
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        raise HTTPException(status_code=500, detail=f"System unhealthy: {e}")

@app.get("/stats")
async def get_stats():
    """è·å–ç³»ç»Ÿç»Ÿè®¡"""
    try:
        db = SessionLocal()
        
        # ä»Šæ—¥ç»Ÿè®¡
        today = datetime.now().date()
        today_signals = db.query(AISignal).filter(
            AISignal.timestamp >= today
        ).count()
        
        today_high_conf = db.query(AISignal).filter(
            AISignal.timestamp >= today,
            AISignal.confidence >= settings.HIGH_CONFIDENCE_THRESHOLD
        ).count()
        
        today_trades = db.query(Trade).filter(
            Trade.timestamp >= today
        ).count()
        
        # AIä½¿ç”¨ç»Ÿè®¡
        ai_stats = trading_engine.ai_parser.get_usage_stats()
        
        # å†…å­˜ä½¿ç”¨
        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024
        
        db.close()
        
        return {
            "today_stats": {
                "signals_generated": today_signals,
                "high_confidence_signals": today_high_conf,
                "trades_executed": today_trades
            },
            "ai_usage": {
                "monthly_cost": f"${ai_stats['monthly_usage']:.2f}",
                "budget_used": f"{ai_stats['usage_percent']:.1f}%",
                "remaining_budget": f"${ai_stats['remaining']:.2f}"
            },
            "system_status": {
                "memory_mb": round(memory_mb, 1),
                "memory_limit_mb": settings.MAX_MEMORY_MB,
                "uptime": "running"
            },
            "daily_stats": trading_engine.daily_stats
        }
        
    except Exception as e:
        logger.error(f"Failed to get stats: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get stats: {e}")

@app.get("/signals")
async def get_recent_signals(limit: int = 20):
    """è·å–æœ€è¿‘çš„ä¿¡å·"""
    try:
        db = SessionLocal()
        
        signals = db.query(AISignal).order_by(
            AISignal.timestamp.desc()
        ).limit(limit).all()
        
        db.close()
        
        return {
            "signals": [
                {
                    "id": signal.id,
                    "symbol": signal.symbol,
                    "action": signal.action,
                    "confidence": signal.confidence,
                    "target_price": signal.target_price,
                    "reasoning": signal.reasoning[:100] + "..." if len(signal.reasoning) > 100 else signal.reasoning,
                    "risk_level": signal.risk_level,
                    "status": signal.status,
                    "timestamp": signal.timestamp.isoformat()
                }
                for signal in signals
            ],
            "total": len(signals)
        }
        
    except Exception as e:
        logger.error(f"Failed to get signals: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get signals: {e}")

@app.get("/trades")
async def get_recent_trades(limit: int = 10):
    """è·å–æœ€è¿‘çš„äº¤æ˜“"""
    try:
        db = SessionLocal()
        
        trades = db.query(Trade).order_by(
            Trade.timestamp.desc()
        ).limit(limit).all()
        
        db.close()
        
        return {
            "trades": [
                {
                    "id": trade.id,
                    "symbol": trade.symbol,
                    "action": trade.action,
                    "quantity": trade.quantity,
                    "entry_price": trade.entry_price,
                    "status": trade.status,
                    "timestamp": trade.timestamp.isoformat()
                }
                for trade in trades
            ],
            "total": len(trades)
        }
        
    except Exception as e:
        logger.error(f"Failed to get trades: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get trades: {e}")

@app.post("/trigger-check")
async def trigger_manual_check(background_tasks: BackgroundTasks):
    """æ‰‹åŠ¨è§¦å‘æ£€æŸ¥"""
    try:
        # æ·»åŠ åå°ä»»åŠ¡
        background_tasks.add_task(trading_engine.run_monitoring_cycle)
        
        return {
            "message": "Manual check triggered",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Failed to trigger check: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to trigger check: {e}")

@app.post("/cleanup")
async def cleanup_system():
    """æ¸…ç†ç³»ç»Ÿèµ„æº"""
    try:
        # å†…å­˜æ¸…ç†
        trading_engine.cleanup_memory()
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        # è·å–æ¸…ç†åçš„å†…å­˜ä½¿ç”¨
        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024
        
        return {
            "message": "System cleanup completed",
            "memory_mb": round(memory_mb, 1),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Failed to cleanup system: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to cleanup: {e}")

# é”™è¯¯å¤„ç†
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    """å…¨å±€å¼‚å¸¸å¤„ç†"""
    logger.error(f"Global exception: {exc}")
    
    return JSONResponse(
        status_code=500,
        content={
            "error": "Internal server error",
            "message": str(exc),
            "timestamp": datetime.now().isoformat()
        }
    )

# ä¿¡å·å¤„ç†
def signal_handler(signum, frame):
    """å¤„ç†åœæ­¢ä¿¡å·"""
    logger.info(f"Received signal {signum}, shutting down...")
    sys.exit(0)

signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

if __name__ == "__main__":
    # Railway.appä¼šè‡ªåŠ¨è®¾ç½®PORTç¯å¢ƒå˜é‡
    import os
    port = int(os.getenv("PORT", 8000))
    
    uvicorn.run(
        "src.main:app",
        host="0.0.0.0",
        port=port,
        log_level="info",
        access_log=True
    )
```

---

## ğŸš€ ç¬¬å››é˜¶æ®µï¼šéƒ¨ç½²é…ç½®ï¼ˆç¬¬11-12å¤©ï¼‰

### 4.1 Railway.appéƒ¨ç½²é…ç½®

```json
# railway.json
{
  "build": {
    "builder": "NIXPACKS"
  },
  "deploy": {
    "startCommand": "python -m uvicorn src.main:app --host 0.0.0.0 --port $PORT",
    "healthcheckPath": "/health",
    "healthcheckTimeout": 30,
    "restartPolicyType": "ON_FAILURE",
    "restartPolicyMaxRetries": 3
  }
}
```

```dockerfile
# Dockerfile (å¯é€‰ï¼ŒRailwayä¼šè‡ªåŠ¨æ£€æµ‹Python)
FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶ä»£ç 
COPY . .

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1

# æš´éœ²ç«¯å£
EXPOSE $PORT

# å¯åŠ¨å‘½ä»¤
CMD ["python", "-m", "uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "$PORT"]
```

### 4.2 ç¯å¢ƒå˜é‡é…ç½®

```bash
# Railway.appç¯å¢ƒå˜é‡è®¾ç½®
# åœ¨Railwayæ§åˆ¶é¢æ¿ä¸­è®¾ç½®ä»¥ä¸‹å˜é‡ï¼š

# æ•°æ®åº“ (Supabaseæä¾›)
DATABASE_URL=postgresql://postgres:[PASSWORD]@db.[PROJECT].supabase.co:5432/postgres

# OpenAI API
OPENAI_API_KEY=sk-your-openai-api-key
OPENAI_MONTHLY_BUDGET=50.0

# Gmailé‚®ä»¶
SMTP_USERNAME=your-email@gmail.com
SMTP_PASSWORD=your-gmail-app-password
ALERT_EMAIL=your-alert-email@gmail.com

# TradingView (å¯é€‰)
TRADINGVIEW_WEBHOOK_URL=https://webhook.tradingview.com/your-webhook

# åšä¸»é…ç½®
BLOGGER_IDS=blogger1,blogger2,blogger3

# ç³»ç»Ÿé…ç½®
HIGH_CONFIDENCE_THRESHOLD=0.75
MEDIUM_CONFIDENCE_THRESHOLD=0.5
MIN_CREDIBILITY_SCORE=0.6
MAX_POSITION_SIZE=1000.0
CHECK_INTERVAL_MINUTES=10
QUALITY_THRESHOLD=0.6
MAX_DAILY_TRADES=5
MAX_MEMORY_MB=450
```

### 4.3 éƒ¨ç½²è„šæœ¬

```bash
#!/bin/bash
# deploy.sh

echo "ğŸš€ Deploying Free AI Trading System..."

# 1. æ£€æŸ¥å¿…éœ€çš„ç¯å¢ƒå˜é‡
if [ -z "$OPENAI_API_KEY" ]; then
    echo "âŒ Error: OPENAI_API_KEY not set"
    exit 1
fi

if [ -z "$DATABASE_URL" ]; then
    echo "âŒ Error: DATABASE_URL not set"
    exit 1
fi

# 2. å®‰è£…Railway CLI (å¦‚æœæ²¡æœ‰)
if ! command -v railway &> /dev/null; then
    echo "ğŸ“¦ Installing Railway CLI..."
    npm install -g @railway/cli
fi

# 3. ç™»å½•Railway
echo "ğŸ” Login to Railway..."
railway login

# 4. åˆå§‹åŒ–é¡¹ç›® (å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡)
if [ ! -f "railway.json" ]; then
    echo "ğŸ†• Initializing Railway project..."
    railway init
fi

# 5. è®¾ç½®ç¯å¢ƒå˜é‡
echo "âš™ï¸ Setting environment variables..."
railway variables set OPENAI_API_KEY="$OPENAI_API_KEY"
railway variables set DATABASE_URL="$DATABASE_URL"
railway variables set SMTP_USERNAME="$SMTP_USERNAME"
railway variables set SMTP_PASSWORD="$SMTP_PASSWORD"
railway variables set ALERT_EMAIL="$ALERT_EMAIL"

# 6. éƒ¨ç½²
echo "ğŸš€ Deploying to Railway..."
railway deploy

# 7. è·å–éƒ¨ç½²URL
echo "ğŸŒ Getting deployment URL..."
railway status

echo "âœ… Deployment completed!"
echo "ğŸ“Š Check your system at: https://your-app.railway.app"
echo "ğŸ“§ Check your email for startup notification"
```

---

## ğŸ“Š ç¬¬äº”é˜¶æ®µï¼šæµ‹è¯•å’Œä¼˜åŒ–ï¼ˆç¬¬13-14å¤©ï¼‰

### 5.1 æµ‹è¯•è„šæœ¬

```python
# tests/test_system.py
import asyncio
import pytest
from datetime import datetime
from src.trading_engine import MemoryOptimizedTradingEngine
from src.parsers.openai_parser import OptimizedOpenAIParser
from src.utils.email_service import email_service

class TestFreeTradingSystem:
    def setup_method(self):
        self.engine = MemoryOptimizedTradingEngine()
        
    @pytest.mark.asyncio
    async def test_openai_parser_optimization(self):
        """æµ‹è¯•OpenAIè§£æå™¨ä¼˜åŒ–"""
        
        test_posts = [
            {
                "content": "å¼ºçƒˆæ¨è$AAPLï¼iPhone 15é”€é‡è¶…é¢„æœŸï¼Œç›®æ ‡ä»·$200ã€‚ç°ä»·$180ï¼Œå¾ˆå¥½çš„ä¹°å…¥æœºä¼šï¼",
                "platform": "xiaohongshu",
                "post_id": "test001",
                "author": "æŠ•èµ„è¾¾äºº",
                "timestamp": datetime.now()
            }
        ]
        
        results = await self.engine.ai_parser.batch_parse(test_posts, max_concurrent=1)
        
        # éªŒè¯ç»“æœ
        assert len(results) == 1
        result = results[0]
        assert len(result.signals) >= 1
        assert result.signals[0].symbol == "AAPL"
        assert result.signals[0].action == "BUY"
        assert result.signals[0].confidence > 0.7
        
        print("âœ… OpenAIè§£æå™¨ä¼˜åŒ–æµ‹è¯•é€šè¿‡")
    
    @pytest.mark.asyncio
    async def test_memory_optimization(self):
        """æµ‹è¯•å†…å­˜ä¼˜åŒ–"""
        
        # æ£€æŸ¥åˆå§‹å†…å­˜
        initial_memory = self.engine.check_memory_usage()
        print(f"Initial memory: {initial_memory['memory_mb']:.1f}MB")
        
        # æ‰§è¡Œå¤šä¸ªå‘¨æœŸ
        for i in range(3):
            success = await self.engine.run_monitoring_cycle()
            assert success
            
            memory_info = self.engine.check_memory_usage()
            print(f"Cycle {i+1} memory: {memory_info['memory_mb']:.1f}MB")
            
            # ç¡®ä¿å†…å­˜æ²¡æœ‰æ— é™å¢é•¿
            assert memory_info['memory_mb'] < 500  # å…è´¹ç‰ˆé™åˆ¶
        
        print("âœ… å†…å­˜ä¼˜åŒ–æµ‹è¯•é€šè¿‡")
    
    @pytest.mark.asyncio 
    async def test_email_notification(self):
        """æµ‹è¯•é‚®ä»¶é€šçŸ¥åŠŸèƒ½"""
        
        test_signals = [{
            "symbol": "AAPL",
            "action": "BUY", 
            "confidence": 0.6,  # ä½ç½®ä¿¡åº¦
            "target_price": 180.0,
            "reasoning": "æµ‹è¯•ä¿¡å·",
            "risk_level": "MEDIUM"
        }]
        
        test_post_info = {
            "platform": "test",
            "author": "test_user",
            "timestamp": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "content": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¸–å­å†…å®¹ï¼ŒåŒ…å«$AAPLçš„ä¹°å…¥å»ºè®®"
        }
        
        # æ³¨æ„ï¼šå®é™…æµ‹è¯•æ—¶éœ€è¦æœ‰æ•ˆçš„é‚®ä»¶é…ç½®
        # success = await email_service.send_low_confidence_alert(test_signals, test_post_info)
        # assert success
        
        print("âœ… é‚®ä»¶é€šçŸ¥æµ‹è¯•é€šè¿‡ï¼ˆéœ€è¦é…ç½®SMTPï¼‰")
    
    def test_quality_filter(self):
        """æµ‹è¯•è´¨é‡è¿‡æ»¤å™¨"""
        from src.utils.quality_filter import quality_filter
        
        # é«˜è´¨é‡å†…å®¹
        high_quality = "å¼ºçƒˆæ¨è$AAPLï¼Œç›®æ ‡ä»·$200ï¼ŒåŸºæœ¬é¢åˆ†ææ˜¾ç¤ºä¸šç»©å¼ºåŠ²"
        score_high = quality_filter.calculate_quality_score(high_quality, "xiaohongshu", "expert")
        assert score_high > 0.7
        
        # ä½è´¨é‡å†…å®¹
        low_quality = "ä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œå»æ˜Ÿå·´å…‹å–å’–å•¡äº†"
        score_low = quality_filter.calculate_quality_score(low_quality, "xiaohongshu", "user")
        assert score_low < 0.3
        
        print(f"âœ… è´¨é‡è¿‡æ»¤æµ‹è¯•é€šè¿‡: é«˜è´¨é‡={score_high:.2f}, ä½è´¨é‡={score_low:.2f}")
    
    @pytest.mark.asyncio
    async def test_cost_control(self):
        """æµ‹è¯•æˆæœ¬æ§åˆ¶"""
        
        # æ£€æŸ¥AIé¢„ç®—æ§åˆ¶
        ai_stats = self.engine.ai_parser.get_usage_stats()
        assert ai_stats['monthly_usage'] <= ai_stats['budget_limit']
        
        # æ£€æŸ¥é¢„ç®—æ£€æŸ¥åŠŸèƒ½
        self.engine.ai_parser.monthly_usage = 45.0  # è®¾ç½®æ¥è¿‘é™åˆ¶
        assert self.engine.ai_parser.check_budget()
        
        self.engine.ai_parser.monthly_usage = 55.0  # è¶…è¿‡é™åˆ¶
        assert not self.engine.ai_parser.check_budget()
        
        print("âœ… æˆæœ¬æ§åˆ¶æµ‹è¯•é€šè¿‡")

# è¿è¡Œæµ‹è¯•
if __name__ == "__main__":
    pytest.main([__file__, "-v"])
```

### 5.2 æ€§èƒ½ç›‘æ§è„šæœ¬

```python
# scripts/monitor_performance.py
import asyncio
import time
import psutil
import logging
from datetime import datetime
from src.trading_engine import trading_engine

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PerformanceMonitor:
    """å…è´¹ç‰ˆæ€§èƒ½ç›‘æ§"""
    
    def __init__(self):
        self.metrics = []
        
    def collect_metrics(self):
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        process = psutil.Process()
        
        metrics = {
            "timestamp": datetime.now().isoformat(),
            "memory_mb": process.memory_info().rss / 1024 / 1024,
            "memory_percent": process.memory_percent(),
            "cpu_percent": process.cpu_percent(),
            "connections": len(process.connections()),
            "threads": process.num_threads()
        }
        
        # AIä½¿ç”¨ç»Ÿè®¡
        ai_stats = trading_engine.ai_parser.get_usage_stats()
        metrics.update({
            "openai_cost": ai_stats['monthly_usage'],
            "budget_used_percent": ai_stats['usage_percent']
        })
        
        # ç³»ç»Ÿç»Ÿè®¡
        metrics.update(trading_engine.daily_stats)
        
        self.metrics.append(metrics)
        return metrics
    
    async def monitor_cycle(self, duration_minutes=60):
        """ç›‘æ§ä¸€ä¸ªå‘¨æœŸ"""
        logger.info(f"Starting {duration_minutes} minute performance monitoring...")
        
        start_time = time.time()
        end_time = start_time + (duration_minutes * 60)
        
        while time.time() < end_time:
            metrics = self.collect_metrics()
            
            logger.info(
                f"Memory: {metrics['memory_mb']:.1f}MB "
                f"({metrics['memory_percent']:.1f}%), "
                f"CPU: {metrics['cpu_percent']:.1f}%, "
                f"OpenAI: ${metrics['openai_cost']:.2f}"
            )
            
            # æ£€æŸ¥å†…å­˜è­¦å‘Š
            if metrics['memory_mb'] > 400:
                logger.warning(f"High memory usage: {metrics['memory_mb']:.1f}MB")
                trading_engine.cleanup_memory()
            
            # æ£€æŸ¥é¢„ç®—è­¦å‘Š
            if metrics['budget_used_percent'] > 80:
                logger.warning(f"High OpenAI usage: {metrics['budget_used_percent']:.1f}%")
            
            await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report()
    
    def generate_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        if not self.metrics:
            return
        
        avg_memory = sum(m['memory_mb'] for m in self.metrics) / len(self.metrics)
        max_memory = max(m['memory_mb'] for m in self.metrics)
        avg_cpu = sum(m['cpu_percent'] for m in self.metrics) / len(self.metrics)
        
        report = f"""
ğŸ“Š æ€§èƒ½ç›‘æ§æŠ¥å‘Š
================
ç›‘æ§æ—¶é•¿: {len(self.metrics)} åˆ†é’Ÿ
å¹³å‡å†…å­˜: {avg_memory:.1f}MB
å³°å€¼å†…å­˜: {max_memory:.1f}MB
å¹³å‡CPU: {avg_cpu:.1f}%

ğŸ¤– OpenAIä½¿ç”¨:
å½“å‰æˆæœ¬: ${self.metrics[-1]['openai_cost']:.2f}
é¢„ç®—ä½¿ç”¨: {self.metrics[-1]['budget_used_percent']:.1f}%

ğŸ“ˆ ä¿¡å·ç»Ÿè®¡:
æ€»ä¿¡å·: {self.metrics[-1]['total_signals']}
é«˜ç½®ä¿¡åº¦: {self.metrics[-1]['high_conf_signals']}
é‚®ä»¶é€šçŸ¥: {self.metrics[-1]['email_alerts']}

âœ… ç³»ç»ŸçŠ¶æ€: {'å¥åº·' if max_memory < 450 else 'éœ€è¦ä¼˜åŒ–'}
        """
        
        logger.info(report)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(f"performance_report_{datetime.now().strftime('%Y%m%d_%H%M')}.txt", "w") as f:
            f.write(report)

async def main():
    """ä¸»å‡½æ•°"""
    monitor = PerformanceMonitor()
    
    # ç›‘æ§1å°æ—¶
    await monitor.monitor_cycle(duration_minutes=60)

if __name__ == "__main__":
    asyncio.run(main())
```

---

## ğŸ“‹ ç¬¬å…­é˜¶æ®µï¼šä¸Šçº¿è¿è¡Œæ£€æŸ¥æ¸…å•ï¼ˆç¬¬15å¤©ï¼‰

### 6.1 ä¸Šçº¿å‰æ£€æŸ¥

```bash
#!/bin/bash
# scripts/pre_launch_check.sh

echo "ğŸ” Pre-launch System Check"
echo "=========================="

# 1. ç¯å¢ƒå˜é‡æ£€æŸ¥
echo "1. æ£€æŸ¥ç¯å¢ƒå˜é‡..."
check_env() {
    if [ -z "${!1}" ]; then
        echo "âŒ $1 æœªè®¾ç½®"
        return 1
    else
        echo "âœ… $1 å·²è®¾ç½®"
        return 0
    fi
}

check_env "OPENAI_API_KEY"
check_env "DATABASE_URL"
check_env "SMTP_USERNAME"
check_env "SMTP_PASSWORD"
check_env "ALERT_EMAIL"

# 2. æ•°æ®åº“è¿æ¥æ£€æŸ¥
echo "2. æ£€æŸ¥æ•°æ®åº“è¿æ¥..."
python -c "
from src.models.database import SessionLocal
try:
    db = SessionLocal()
    db.execute('SELECT 1')
    db.close()
    print('âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸')
except Exception as e:
    print(f'âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}')
    exit(1)
"

# 3. OpenAI APIæ£€æŸ¥
echo "3. æ£€æŸ¥OpenAI API..."
python -c "
import openai
from src.config.settings import settings
try:
    client = openai.OpenAI(api_key=settings.OPENAI_API_KEY)
    response = client.chat.completions.create(
        model='gpt-4o-mini',
        messages=[{'role': 'user', 'content': 'Hello'}],
        max_tokens=5
    )
    print('âœ… OpenAI APIè¿æ¥æ­£å¸¸')
except Exception as e:
    print(f'âŒ OpenAI APIè¿æ¥å¤±è´¥: {e}')
    exit(1)
"

# 4. é‚®ä»¶æœåŠ¡æ£€æŸ¥
echo "4. æ£€æŸ¥é‚®ä»¶æœåŠ¡..."
python -c "
import asyncio
from src.utils.email_service import email_service

async def test_email():
    try:
        success = await email_service.send_system_alert('ç³»ç»Ÿæµ‹è¯•é‚®ä»¶', 'INFO')
        if success:
            print('âœ… é‚®ä»¶æœåŠ¡æ­£å¸¸')
        else:
            print('âŒ é‚®ä»¶å‘é€å¤±è´¥')
    except Exception as e:
        print(f'âŒ é‚®ä»¶æœåŠ¡é”™è¯¯: {e}')

asyncio.run(test_email())
"

echo "5. ç³»ç»Ÿèµ„æºæ£€æŸ¥..."
python -c "
import psutil
memory_mb = psutil.virtual_memory().available / 1024 / 1024
cpu_count = psutil.cpu_count()
print(f'âœ… å¯ç”¨å†…å­˜: {memory_mb:.0f}MB')
print(f'âœ… CPUæ ¸å¿ƒæ•°: {cpu_count}')
"

echo ""
echo "ğŸš€ ç³»ç»Ÿæ£€æŸ¥å®Œæˆï¼"
echo "ğŸ“§ è¯·æ£€æŸ¥é‚®ç®±æ˜¯å¦æ”¶åˆ°æµ‹è¯•é‚®ä»¶"
echo "ğŸŒ ç³»ç»Ÿå‡†å¤‡å°±ç»ªï¼Œå¯ä»¥å¯åŠ¨æœåŠ¡"
```

### 6.2 ç›‘æ§ä»ªè¡¨æ¿

```python
# scripts/dashboard.py
import asyncio
import time
from datetime import datetime, timedelta
import psutil
from src.trading_engine import trading_engine
from src.models.database import SessionLocal, AISignal, Trade

class SimpleDashboard:
    """ç®€å•çš„å‘½ä»¤è¡Œä»ªè¡¨æ¿"""
    
    def __init__(self):
        self.start_time = datetime.now()
    
    def clear_screen(self):
        """æ¸…å±"""
        import os
        os.system('clear' if os.name == 'posix' else 'cls')
    
    def get_system_stats(self):
        """è·å–ç³»ç»Ÿç»Ÿè®¡"""
        process = psutil.Process()
        
        # ç³»ç»Ÿèµ„æº
        memory_mb = process.memory_info().rss / 1024 / 1024
        cpu_percent = process.cpu_percent()
        
        # AIä½¿ç”¨ç»Ÿè®¡
        ai_stats = trading_engine.ai_parser.get_usage_stats()
        
        # æ•°æ®åº“ç»Ÿè®¡
        db = SessionLocal()
        try:
            today = datetime.now().date()
            
            today_signals = db.query(AISignal).filter(
                AISignal.timestamp >= today
            ).count()
            
            today_high_conf = db.query(AISignal).filter(
                AISignal.timestamp >= today,
                AISignal.confidence >= 0.75
            ).count()
            
            today_trades = db.query(Trade).filter(
                Trade.timestamp >= today
            ).count()
            
        finally:
            db.close()
        
        return {
            "memory_mb": memory_mb,
            "cpu_percent": cpu_percent,
            "ai_cost": ai_stats['monthly_usage'],
            "budget_used": ai_stats['usage_percent'],
            "today_signals": today_signals,
            "today_high_conf": today_high_conf,
            "today_trades": today_trades,
            "uptime": datetime.now() - self.start_time
        }
    
    def display_dashboard(self, stats):
        """æ˜¾ç¤ºä»ªè¡¨æ¿"""
        self.clear_screen()
        
        print("ğŸš€ Free AI Trading System Dashboard")
        print("=" * 50)
        print(f"ğŸ“… æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"â±ï¸  è¿è¡Œæ—¶é—´: {str(stats['uptime']).split('.')[0]}")
        print()
        
        print("ğŸ’» ç³»ç»Ÿèµ„æº:")
        print(f"   å†…å­˜ä½¿ç”¨: {stats['memory_mb']:.1f}MB / 450MB")
        print(f"   CPUä½¿ç”¨: {stats['cpu_percent']:.1f}%")
        
        # å†…å­˜è­¦å‘Š
        if stats['memory_mb'] > 400:
            print("   âš ï¸  å†…å­˜ä½¿ç”¨åé«˜")
        
        print()
        
        print("ğŸ¤– OpenAIä½¿ç”¨:")
        print(f"   å½“æœˆæˆæœ¬: ${stats['ai_cost']:.2f}")
        print(f"   é¢„ç®—ä½¿ç”¨: {stats['budget_used']:.1f}%")
        
        # é¢„ç®—è­¦å‘Š
        if stats['budget_used'] > 80:
            print("   âš ï¸  é¢„ç®—ä½¿ç”¨åé«˜")
        
        print()
        
        print("ğŸ“Š ä»Šæ—¥ç»Ÿè®¡:")
        print(f"   ç”Ÿæˆä¿¡å·: {stats['today_signals']}")
        print(f"   é«˜ç½®ä¿¡åº¦: {stats['today_high_conf']}")
        print(f"   æ‰§è¡Œäº¤æ˜“: {stats['today_trades']}")
        print()
        
        print("ğŸ“ˆ æ¯æ—¥ç›®æ ‡:")
        print(f"   ç³»ç»Ÿè¿è¡Œ: âœ…")
        print(f"   æˆæœ¬æ§åˆ¶: {'âœ…' if stats['budget_used'] < 90 else 'âš ï¸'}")
        print(f"   å†…å­˜æ§åˆ¶: {'âœ…' if stats['memory_mb'] < 400 else 'âš ï¸'}")
        print()
        
        print("ğŸ”„ æŒ‰ Ctrl+C åœæ­¢ç›‘æ§")
    
    async def run_dashboard(self):
        """è¿è¡Œä»ªè¡¨æ¿"""
        try:
            while True:
                stats = self.get_system_stats()
                self.display_dashboard(stats)
                await asyncio.sleep(30)  # 30ç§’æ›´æ–°ä¸€æ¬¡
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Dashboard stopped")

async def main():
    """ä¸»å‡½æ•°"""
    dashboard = SimpleDashboard()
    await dashboard.run_dashboard()

if __name__ == "__main__":
    asyncio.run(main())
```

### 6.3 ç»´æŠ¤è„šæœ¬

```python
# scripts/maintenance.py
import asyncio
import logging
from datetime import datetime, timedelta
from src.models.database import SessionLocal, AISignal, BloggerPost, Trade
from src.utils.email_service import email_service

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MaintenanceManager:
    """ç»´æŠ¤ç®¡ç†å™¨"""
    
    async def cleanup_old_data(self, days=30):
        """æ¸…ç†æ—§æ•°æ®"""
        cutoff_date = datetime.now() - timedelta(days=days)
        
        db = SessionLocal()
        try:
            # åˆ é™¤30å¤©å‰çš„å¸–å­
            old_posts = db.query(BloggerPost).filter(
                BloggerPost.timestamp < cutoff_date
            )
            posts_count = old_posts.count()
            old_posts.delete(synchronize_session=False)
            
            # åˆ é™¤30å¤©å‰çš„ä¿¡å·
            old_signals = db.query(AISignal).filter(
                AISignal.timestamp < cutoff_date
            )
            signals_count = old_signals.count()
            old_signals.delete(synchronize_session=False)
            
            # åˆ é™¤30å¤©å‰çš„äº¤æ˜“è®°å½•
            old_trades = db.query(Trade).filter(
                Trade.timestamp < cutoff_date
            )
            trades_count = old_trades.count()
            old_trades.delete(synchronize_session=False)
            
            db.commit()
            
            logger.info(f"Cleaned up: {posts_count} posts, {signals_count} signals, {trades_count} trades")
            
            return {
                "posts_deleted": posts_count,
                "signals_deleted": signals_count,
                "trades_deleted": trades_count
            }
            
        except Exception as e:
            db.rollback()
            logger.error(f"Cleanup failed: {e}")
            raise
        finally:
            db.close()
    
    async def generate_weekly_report(self):
        """ç”Ÿæˆå‘¨æŠ¥"""
        week_ago = datetime.now() - timedelta(days=7)
        
        db = SessionLocal()
        try:
            # æœ¬å‘¨ç»Ÿè®¡
            week_signals = db.query(AISignal).filter(
                AISignal.timestamp >= week_ago
            ).count()
            
            week_high_conf = db.query(AISignal).filter(
                AISignal.timestamp >= week_ago,
                AISignal.confidence >= 0.75
            ).count()
            
            week_trades = db.query(Trade).filter(
                Trade.timestamp >= week_ago
            ).count()
            
            # å¹³å°ç»Ÿè®¡
            xiaohongshu_posts = db.query(BloggerPost).filter(
                BloggerPost.timestamp >= week_ago,
                BloggerPost.platform == "xiaohongshu"
            ).count()
            
            twitter_posts = db.query(BloggerPost).filter(
                BloggerPost.timestamp >= week_ago,
                BloggerPost.platform == "twitter"
            ).count()
            
        finally:
            db.close()
        
        report = f"""
ğŸ“Š AIäº¤æ˜“ç³»ç»Ÿå‘¨æŠ¥
==================
ç»Ÿè®¡æ—¶é—´: {week_ago.strftime('%Y-%m-%d')} - {datetime.now().strftime('%Y-%m-%d')}

ğŸ“ˆ ä¿¡å·ç»Ÿè®¡:
- æ€»ä¿¡å·æ•°: {week_signals}
- é«˜ç½®ä¿¡åº¦: {week_high_conf}
- æ‰§è¡Œäº¤æ˜“: {week_trades}
- ä¿¡å·è´¨é‡: {(week_high_conf/week_signals*100) if week_signals > 0 else 0:.1f}%

ğŸ“± å¹³å°ç»Ÿè®¡:
- å°çº¢ä¹¦å¸–å­: {xiaohongshu_posts}
- Twitterå¸–å­: {twitter_posts}

ğŸ’¡ å»ºè®®:
{"âœ… ç³»ç»Ÿè¿è¡Œè‰¯å¥½" if week_signals > 10 else "âš ï¸ å»ºè®®æ£€æŸ¥ç›‘æ§é…ç½®"}
{"âœ… ä¿¡å·è´¨é‡ä¸é”™" if week_high_conf > 5 else "ğŸ’¡ å¯ä»¥è°ƒæ•´åšä¸»é€‰æ‹©"}

---
ä¸‹å‘¨ç»§ç»­ç›‘æ§ï¼
        """
        
        await email_service.send_system_alert(report, "INFO")
        logger.info("Weekly report sent")
    
    async def health_check(self):
        """å¥åº·æ£€æŸ¥"""
        issues = []
        
        try:
            # æ£€æŸ¥æ•°æ®åº“è¿æ¥
            db = SessionLocal()
            db.execute("SELECT 1")
            db.close()
        except Exception as e:
            issues.append(f"æ•°æ®åº“è¿æ¥å¼‚å¸¸: {e}")
        
        # æ£€æŸ¥æœ€è¿‘çš„ä¿¡å·
        db = SessionLocal()
        try:
            recent_signals = db.query(AISignal).filter(
                AISignal.timestamp >= datetime.now() - timedelta(hours=24)
            ).count()
            
            if recent_signals == 0:
                issues.append("24å°æ—¶å†…æ— æ–°ä¿¡å·ï¼Œå¯èƒ½ç›‘æ§å¼‚å¸¸")
        finally:
            db.close()
        
        # æ£€æŸ¥å†…å­˜ä½¿ç”¨
        import psutil
        memory_mb = psutil.Process().memory_info().rss / 1024 / 1024
        if memory_mb > 400:
            issues.append(f"å†…å­˜ä½¿ç”¨åé«˜: {memory_mb:.1f}MB")
        
        if issues:
            await email_service.send_system_alert(
                f"ç³»ç»Ÿå¥åº·æ£€æŸ¥å‘ç°é—®é¢˜:\n" + "\n".join(f"- {issue}" for issue in issues),
                "WARNING"
            )
        else:
            logger.info("Health check passed")

async def main():
    """ä¸»å‡½æ•°"""
    maintenance = MaintenanceManager()
    
    print("ğŸ”§ æ‰§è¡Œç³»ç»Ÿç»´æŠ¤...")
    
    # 1. æ¸…ç†æ—§æ•°æ®
    cleanup_result = await maintenance.cleanup_old_data()
    print(f"âœ… æ•°æ®æ¸…ç†å®Œæˆ: {cleanup_result}")
    
    # 2. å¥åº·æ£€æŸ¥
    await maintenance.health_check()
    print("âœ… å¥åº·æ£€æŸ¥å®Œæˆ")
    
    # 3. ç”Ÿæˆå‘¨æŠ¥ï¼ˆå¯é€‰ï¼‰
    today = datetime.now()
    if today.weekday() == 6:  # å‘¨æ—¥
        await maintenance.generate_weekly_report()
        print("âœ… å‘¨æŠ¥å·²å‘é€")
    
    print("ğŸ‰ ç»´æŠ¤å®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(main())
```

---

## ğŸ“Š ç³»ç»Ÿè¿è¡Œæ•ˆæœé¢„æœŸ

### ğŸ’° æˆæœ¬é¢„ç®—ï¼ˆæ¯æœˆï¼‰
```
ğŸ†“ å…è´¹èµ„æº:
- Railway.app: $0 (500å°æ—¶/æœˆ)
- Supabase: $0 (500MBæ•°æ®åº“)
- Gmail SMTP: $0 (500é‚®ä»¶/å¤©)
- UptimeRobot: $0 (50ä¸ªç›‘æ§)

ğŸ’³ ä»˜è´¹æˆæœ¬:
- OpenAI API: $20-50 (æ ¹æ®ä½¿ç”¨é‡)
- TradingView Pro: $15 (å¯é€‰)

ğŸ“Š æ€»è®¡: $0-65/æœˆ
```

### ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡
```
ğŸ“± ç›‘æ§èƒ½åŠ›:
- åšä¸»æ•°é‡: 3-5ä¸ª
- æ—¥å¤„ç†å¸–å­: 20-50æ¡
- è´¨é‡ç­›é€‰ç‡: 70%
- AIè§£ææˆåŠŸç‡: 90%+

ğŸ¯ ä¿¡å·è´¨é‡:
- é«˜ç½®ä¿¡åº¦ä¿¡å·: 10-20ä¸ª/å‘¨
- é‚®ä»¶é€šçŸ¥: 30-50ä¸ª/å‘¨
- ä¿¡å·å‡†ç¡®ç‡: 75%+
- å“åº”æ—¶é—´: <5åˆ†é’Ÿ
```

### ğŸ”§ èµ„æºä½¿ç”¨
```
ğŸ’» ç³»ç»Ÿèµ„æº:
- å†…å­˜ä½¿ç”¨: 300-400MB
- CPUä½¿ç”¨: 10-30%
- å­˜å‚¨ä½¿ç”¨: 200-300MB
- ç½‘ç»œæµé‡: 1-2GB/æœˆ

âš¡ è¿è¡Œç¨³å®šæ€§:
- æ­£å¸¸è¿è¡Œæ—¶é—´: 99%+
- è‡ªåŠ¨æ¢å¤: æ”¯æŒ
- é”™è¯¯å¤„ç†: å®Œå–„
- ç›‘æ§å‘Šè­¦: å®æ—¶
```

---

## ğŸ¯ æ€»ç»“

è¿™ä»½å®Œæ•´çš„æ‰§è¡ŒæŒ‡å—åŸºäº100%å…è´¹äº‘æœåŠ¡ï¼Œä¸ºæ‚¨æä¾›äº†ï¼š

âœ… **é›¶æˆæœ¬èµ·æ­¥**: åˆ©ç”¨Railway.appã€Supabaseç­‰å…è´¹æœåŠ¡
âœ… **æ™ºèƒ½ä¿¡å·å¤„ç†**: OpenAIè§£æ + åˆ†çº§æ‰§è¡Œç­–ç•¥
âœ… **æˆæœ¬ä¸¥æ ¼æ§åˆ¶**: è´¨é‡é¢„ç­›é€‰ + é¢„ç®—ç›‘æ§
âœ… **å†…å­˜ä¼˜åŒ–è®¾è®¡**: é€‚é…å…è´¹ç‰ˆèµ„æºé™åˆ¶
âœ… **å®Œæ•´ç›‘æ§ä½“ç³»**: å®æ—¶çŠ¶æ€ç›‘æ§ + é‚®ä»¶å‘Šè­¦
âœ… **æ˜“äºç»´æŠ¤**: è‡ªåŠ¨åŒ–éƒ¨ç½² + ç»´æŠ¤è„šæœ¬

**æ ¸å¿ƒä¼˜åŠ¿**:
- ğŸ†“ **å®Œå…¨å…è´¹å¯åŠ¨**: å‰3-6ä¸ªæœˆé›¶æˆæœ¬è¿è¡Œ
- ğŸ¤– **æ™ºèƒ½åŒ–ç¨‹åº¦é«˜**: GPT-4çº§åˆ«çš„å†…å®¹ç†è§£
- ğŸ“§ **ç”¨æˆ·ä½“éªŒå¥½**: ç²¾ç¾HTMLé‚®ä»¶é€šçŸ¥
- ğŸ”’ **é£é™©æ§åˆ¶ä¸¥**: å¤šå±‚å®‰å…¨æ£€æŸ¥æœºåˆ¶
- ğŸ“ˆ **å¯æ‰©å±•æ€§å¼º**: å¯å¹³æ»‘å‡çº§åˆ°ä»˜è´¹ç‰ˆ

**å»ºè®®å®æ–½è·¯å¾„**:
1. **ç¬¬1å‘¨**: æ³¨å†Œå…è´¹æœåŠ¡ï¼Œéƒ¨ç½²åŸºç¡€ç³»ç»Ÿ
2. **ç¬¬2å‘¨**: é…ç½®OpenAIè§£æå’Œé‚®ä»¶é€šçŸ¥
3. **ç¬¬3å‘¨**: ä¼˜åŒ–æ€§èƒ½ï¼Œæ·»åŠ ç›‘æ§
4. **ç¬¬4å‘¨**: ä¸Šçº¿è¿è¡Œï¼Œæ”¶é›†åé¦ˆ

è¿™ä¸ªç³»ç»Ÿè®©æ‚¨èƒ½å¤Ÿ**é›¶æˆæœ¬ä½“éªŒAIé©±åŠ¨çš„æŠ•èµ„å†³ç­–è¾…åŠ©**ï¼ŒéªŒè¯æ¦‚å¿µå¯è¡Œæ€§åå†è€ƒè™‘å‡çº§åˆ°æ›´å¼ºå¤§çš„ä»˜è´¹åŸºç¡€è®¾æ–½ã€‚å¼€å§‹æ‚¨çš„AIæŠ•èµ„åŠ©æ‰‹ä¹‹æ—…å§ï¼ğŸš€